D:\conda\envs\nmt\python.exe D:/nlper/nmt/fairseq/train.py data-bin/iwslt14 --max-tokens 4096 --user-dir moe_extension --task moe_translation --arch moe_transformer_iwslt_de_en --moe-freq 2 --moe-expert-count 4 --fp16 --optimizer adam --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.2 --weight-decay 0.0001 --criterion moe_label_smoothed_cross_entropy --moe-gate-loss-wt 0.01 --label-smoothing 0.1 --eval-bleu --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --num-workers 0
2022-12-16 18:31:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'moe_extension', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='moe_transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='moe_transformer_iwslt_de_en', attention_dropout=0.0, aux_w=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cmr_gate_loss_p=1.0, cmr_gate_loss_wt=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='moe_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, der_coef=0.5, der_eps=0.1, der_tau=1.0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, entity_dict=None, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, moe_cmr_xgpu=False, moe_expert_count=4, moe_freq=2, moe_gate_loss_combine_method='average', moe_gate_loss_transform='none', moe_gate_loss_wt=0.01, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=0, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='moe_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, topk=-1, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_der=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='moe_extension', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'moe_translation', 'data': 'data-bin/iwslt14', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'use_der': False, 'entity_dict': None, 'topk': -1, 'der_coef': 0.5, 'der_tau': 1.0, 'der_eps': 0.1, 'aux_w': 0.0}, 'criterion': {'_name': 'moe_label_smoothed_cross_entropy', 'moe_gate_loss_wt': 0.01, 'moe_gate_loss_combine_method': 'average', 'moe_gate_loss_transform': 'none', 'sentence_avg': False, 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'cmr_gate_loss_p': 1.0, 'cmr_gate_loss_wt': 0.0, 'moe_cmr_xgpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-12-16 18:31:15 | INFO | moe_extension.tasks.moe_translation | [de] dictionary: 10152 types
2022-12-16 18:31:15 | INFO | moe_extension.tasks.moe_translation | [en] dictionary: 10152 types
2022-12-16 18:31:16 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg): Linear(in_features=512, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg): Linear(in_features=512, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg): Linear(in_features=512, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg): Linear(in_features=512, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg): Linear(in_features=512, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg): Linear(in_features=512, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=512, out_features=1024, bias=True)
              (fc2): Linear(in_features=1024, out_features=512, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-16 18:31:16 | INFO | fairseq_cli.train | task: MoETranslationTask
2022-12-16 18:31:16 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-16 18:31:16 | INFO | fairseq_cli.train | criterion: MoELabelSmoothedCrossEntropyCriterion
2022-12-16 18:31:16 | INFO | fairseq_cli.train | num. shared model params: 40,848,384 (num. trained: 40,848,384)
2022-12-16 18:31:16 | INFO | fairseq_cli.train | num. expert model params: 25202688 (num. trained: 25202688)
2022-12-16 18:31:16 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14\valid.de-en.de
2022-12-16 18:31:16 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14\valid.de-en.en
2022-12-16 18:31:16 | INFO | moe_extension.tasks.moe_translation | data-bin/iwslt14 valid de-en 7283 examples
2022-12-16 18:31:16 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- encoder.layers.3.moe_layer.gate.wg.bias
2022-12-16 18:31:16 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- encoder.layers.5.moe_layer.gate.wg.bias
2022-12-16 18:31:16 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.layers.1.moe_layer.gate.wg.bias
2022-12-16 18:31:16 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.layers.3.moe_layer.gate.wg.bias
2022-12-16 18:31:16 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.layers.5.moe_layer.gate.wg.bias
2022-12-16 18:31:16 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.output_projection.bias
2022-12-16 18:31:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-16 18:31:16 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 12.000 GB ; name = NVIDIA GeForce RTX 3060
2022-12-16 18:31:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-16 18:31:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-16 18:31:16 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2022-12-16 18:31:16 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints\checkpoint_last.pt
2022-12-16 18:31:16 | INFO | fairseq.trainer | No existing checkpoint found checkpoints\checkpoint_last.pt
2022-12-16 18:31:16 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-16 18:31:16 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14\train.de-en.de
2022-12-16 18:31:16 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14\train.de-en.en
2022-12-16 18:31:16 | INFO | moe_extension.tasks.moe_translation | data-bin/iwslt14 train de-en 160239 examples
2022-12-16 18:31:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 18:31:18 | INFO | fairseq.trainer | begin training epoch 1
2022-12-16 18:31:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 18:31:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-16 18:31:40 | INFO | train_inner | epoch 001:    101 / 1102 loss=12.639, nll_loss=12.459, moe_gate_loss=1.2363, cmr_gate_loss=0, overflow_expert1=8.089, overflow_expert2=23.268, entropy_gating=1.298, expert1_balance_top=55.8, expert1_balance_bottom=3.566, unused_expert1_count=0.023, expert2_balance_top=29.637, expert2_balance_bottom=18.02, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5629.58, wps=16441.1, ups=4.68, wpb=3505.2, bsz=119.7, num_updates=100, lr=1.25e-05, gnorm=3.61, loss_scale=64, train_wall=22, gb_free=8.7, wall=24
2022-12-16 18:32:01 | INFO | train_inner | epoch 001:    201 / 1102 loss=10.972, nll_loss=10.597, moe_gate_loss=1.15909, cmr_gate_loss=0, overflow_expert1=3.183, overflow_expert2=23.221, entropy_gating=1.312, expert1_balance_top=47.793, expert1_balance_bottom=6.006, unused_expert1_count=0.02, expert2_balance_top=28.12, expert2_balance_bottom=20.3, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=1549.41, wps=16571.6, ups=4.65, wpb=3564.4, bsz=142.1, num_updates=200, lr=2.5e-05, gnorm=1.66, loss_scale=64, train_wall=21, gb_free=8.7, wall=45
2022-12-16 18:32:23 | INFO | train_inner | epoch 001:    301 / 1102 loss=10.111, nll_loss=9.606, moe_gate_loss=1.07975, cmr_gate_loss=0, overflow_expert1=2.137, overflow_expert2=14.487, entropy_gating=1.312, expert1_balance_top=40.774, expert1_balance_bottom=12.291, unused_expert1_count=0, expert2_balance_top=27.937, expert2_balance_bottom=21.153, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=779.45, wps=16684, ups=4.68, wpb=3563.5, bsz=133.4, num_updates=300, lr=3.75e-05, gnorm=1.638, loss_scale=64, train_wall=21, gb_free=9, wall=66
2022-12-16 18:32:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-16 18:32:45 | INFO | train_inner | epoch 001:    402 / 1102 loss=9.584, nll_loss=8.971, moe_gate_loss=1.05888, cmr_gate_loss=0, overflow_expert1=0.848, overflow_expert2=13.607, entropy_gating=1.298, expert1_balance_top=38.431, expert1_balance_bottom=13.008, unused_expert1_count=0, expert2_balance_top=27.742, expert2_balance_bottom=21.509, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=501.98, wps=16873.9, ups=4.59, wpb=3680, bsz=159, num_updates=400, lr=5e-05, gnorm=1.777, loss_scale=32, train_wall=21, gb_free=8.6, wall=88
2022-12-16 18:33:06 | INFO | train_inner | epoch 001:    502 / 1102 loss=9.358, nll_loss=8.692, moe_gate_loss=1.0645, cmr_gate_loss=0, overflow_expert1=0.535, overflow_expert2=13.691, entropy_gating=1.255, expert1_balance_top=37.137, expert1_balance_bottom=14.019, unused_expert1_count=0, expert2_balance_top=27.817, expert2_balance_bottom=21.73, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=413.56, wps=16637.5, ups=4.58, wpb=3635.6, bsz=152.6, num_updates=500, lr=6.25e-05, gnorm=1.811, loss_scale=32, train_wall=22, gb_free=9, wall=110
2022-12-16 18:33:28 | INFO | train_inner | epoch 001:    602 / 1102 loss=9.033, nll_loss=8.313, moe_gate_loss=1.0706, cmr_gate_loss=0, overflow_expert1=0.303, overflow_expert2=14.427, entropy_gating=1.203, expert1_balance_top=36.782, expert1_balance_bottom=14.187, unused_expert1_count=0, expert2_balance_top=28.76, expert2_balance_bottom=20.725, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=318.04, wps=16808.3, ups=4.67, wpb=3597.4, bsz=157.4, num_updates=600, lr=7.5e-05, gnorm=1.863, loss_scale=32, train_wall=21, gb_free=8.9, wall=132
2022-12-16 18:33:49 | INFO | train_inner | epoch 001:    702 / 1102 loss=8.776, nll_loss=8.013, moe_gate_loss=1.06612, cmr_gate_loss=0, overflow_expert1=0.174, overflow_expert2=14.523, entropy_gating=1.171, expert1_balance_top=36.051, expert1_balance_bottom=14.772, unused_expert1_count=0, expert2_balance_top=29.655, expert2_balance_bottom=19.936, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=258.23, wps=16547.9, ups=4.64, wpb=3567.7, bsz=160.9, num_updates=700, lr=8.75e-05, gnorm=1.876, loss_scale=32, train_wall=21, gb_free=8.7, wall=153
2022-12-16 18:34:11 | INFO | train_inner | epoch 001:    802 / 1102 loss=8.659, nll_loss=7.877, moe_gate_loss=1.05203, cmr_gate_loss=0, overflow_expert1=0.079, overflow_expert2=12.169, entropy_gating=1.139, expert1_balance_top=34.336, expert1_balance_bottom=16.026, unused_expert1_count=0, expert2_balance_top=30.247, expert2_balance_bottom=20.111, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=235.07, wps=16678.9, ups=4.65, wpb=3587.1, bsz=137, num_updates=800, lr=0.0001, gnorm=1.686, loss_scale=32, train_wall=21, gb_free=8.8, wall=175
2022-12-16 18:34:32 | INFO | train_inner | epoch 001:    902 / 1102 loss=8.461, nll_loss=7.648, moe_gate_loss=1.04851, cmr_gate_loss=0, overflow_expert1=0.081, overflow_expert2=11.911, entropy_gating=1.122, expert1_balance_top=34.078, expert1_balance_bottom=15.957, unused_expert1_count=0, expert2_balance_top=30.681, expert2_balance_bottom=19.834, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=200.54, wps=16608.7, ups=4.68, wpb=3547.7, bsz=140.4, num_updates=900, lr=0.0001125, gnorm=1.614, loss_scale=32, train_wall=21, gb_free=8.9, wall=196
2022-12-16 18:34:54 | INFO | train_inner | epoch 001:   1002 / 1102 loss=8.294, nll_loss=7.455, moe_gate_loss=1.04819, cmr_gate_loss=0, overflow_expert1=0.05, overflow_expert2=11.383, entropy_gating=1.103, expert1_balance_top=34.096, expert1_balance_bottom=15.728, unused_expert1_count=0, expert2_balance_top=30.408, expert2_balance_bottom=20.074, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=175.5, wps=16653.2, ups=4.62, wpb=3601.3, bsz=154.1, num_updates=1000, lr=0.000125, gnorm=1.723, loss_scale=32, train_wall=21, gb_free=8.9, wall=218
2022-12-16 18:35:15 | INFO | train_inner | epoch 001:   1102 / 1102 loss=8.21, nll_loss=7.356, moe_gate_loss=1.05429, cmr_gate_loss=0, overflow_expert1=0.343, overflow_expert2=11.698, entropy_gating=1.086, expert1_balance_top=35.474, expert1_balance_bottom=14.965, unused_expert1_count=0, expert2_balance_top=31.004, expert2_balance_bottom=19.753, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=163.85, wps=16731.2, ups=4.7, wpb=3561.5, bsz=141.5, num_updates=1100, lr=0.0001375, gnorm=1.68, loss_scale=32, train_wall=21, gb_free=8.9, wall=239
2022-12-16 18:35:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 18:35:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:36:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:36:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:36:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:39:38 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.053 | nll_loss 7.165 | moe_gate_loss 1.03365 | cmr_gate_loss 0 | overflow_expert1 11.653 | overflow_expert2 88.339 | entropy_gating 1.097 | expert1_balance_top 33.749 | expert1_balance_bottom 16.844 | unused_expert1_count 0 | expert2_balance_top 31.167 | expert2_balance_bottom 19.154 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 143.54 | bleu 4.89 | wps 674.6 | wpb 2835.3 | bsz 115.6 | num_updates 1100
2022-12-16 18:39:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1100 updates
2022-12-16 18:39:38 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint1.pt
2022-12-16 18:39:39 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint1.pt
2022-12-16 18:39:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint1.pt (epoch 1 @ 1100 updates, score 4.89) (writing took 3.08392630000003 seconds)
2022-12-16 18:39:41 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-12-16 18:39:41 | INFO | train | epoch 001 | loss 9.457 | nll_loss 8.81 | moe_gate_loss 1.0849 | cmr_gate_loss 0 | overflow_expert1 1.438 | overflow_expert2 14.944 | entropy_gating 1.132 | expert1_balance_top 39.159 | expert1_balance_bottom 12.775 | unused_expert1_count 0.004 | expert2_balance_top 29.273 | expert2_balance_bottom 20.286 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 448.86 | wps 7841.8 | ups 2.19 | wpb 3582.9 | bsz 145.3 | num_updates 1100 | lr 0.0001375 | gnorm 1.903 | loss_scale 32 | train_wall 234 | gb_free 8.9 | wall 505
2022-12-16 18:39:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 18:39:41 | INFO | fairseq.trainer | begin training epoch 2
2022-12-16 18:39:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 18:40:04 | INFO | train_inner | epoch 002:    100 / 1102 loss=7.946, nll_loss=7.055, moe_gate_loss=1.03967, cmr_gate_loss=0, overflow_expert1=0.07, overflow_expert2=9.559, entropy_gating=1.07, expert1_balance_top=32.867, expert1_balance_bottom=16.428, unused_expert1_count=0, expert2_balance_top=30.911, expert2_balance_bottom=19.911, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=132.97, wps=1262.9, ups=0.35, wpb=3646.3, bsz=162, num_updates=1200, lr=0.00015, gnorm=1.614, loss_scale=32, train_wall=22, gb_free=8.9, wall=528
2022-12-16 18:40:26 | INFO | train_inner | epoch 002:    200 / 1102 loss=7.95, nll_loss=7.056, moe_gate_loss=1.04405, cmr_gate_loss=0, overflow_expert1=0.04, overflow_expert2=10.564, entropy_gating=1.059, expert1_balance_top=33.534, expert1_balance_bottom=15.655, unused_expert1_count=0, expert2_balance_top=31.644, expert2_balance_bottom=19.705, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=133.08, wps=16329.6, ups=4.56, wpb=3581, bsz=144.6, num_updates=1300, lr=0.0001625, gnorm=1.704, loss_scale=32, train_wall=22, gb_free=8.7, wall=550
2022-12-16 18:40:48 | INFO | train_inner | epoch 002:    300 / 1102 loss=7.769, nll_loss=6.85, moe_gate_loss=1.0497, cmr_gate_loss=0, overflow_expert1=0.214, overflow_expert2=10.253, entropy_gating=1.03, expert1_balance_top=34.271, expert1_balance_bottom=15.308, unused_expert1_count=0, expert2_balance_top=32.205, expert2_balance_bottom=19.451, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=115.33, wps=16169.4, ups=4.57, wpb=3540.3, bsz=161.2, num_updates=1400, lr=0.000175, gnorm=1.773, loss_scale=32, train_wall=22, gb_free=8.8, wall=571
2022-12-16 18:41:09 | INFO | train_inner | epoch 002:    400 / 1102 loss=7.737, nll_loss=6.81, moe_gate_loss=1.04754, cmr_gate_loss=0, overflow_expert1=0.01, overflow_expert2=9.052, entropy_gating=1.009, expert1_balance_top=32.874, expert1_balance_bottom=15.123, unused_expert1_count=0, expert2_balance_top=32.468, expert2_balance_bottom=19.814, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=112.19, wps=16443.8, ups=4.61, wpb=3570.1, bsz=129.4, num_updates=1500, lr=0.0001875, gnorm=1.449, loss_scale=32, train_wall=21, gb_free=8.9, wall=593
2022-12-16 18:41:31 | INFO | train_inner | epoch 002:    500 / 1102 loss=7.49, nll_loss=6.529, moe_gate_loss=1.04264, cmr_gate_loss=0, overflow_expert1=0.008, overflow_expert2=9.316, entropy_gating=0.998, expert1_balance_top=33.169, expert1_balance_bottom=15.589, unused_expert1_count=0, expert2_balance_top=33.107, expert2_balance_bottom=19.566, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=92.35, wps=16590.5, ups=4.6, wpb=3603.8, bsz=140.3, num_updates=1600, lr=0.0002, gnorm=1.496, loss_scale=32, train_wall=21, gb_free=8.8, wall=615
2022-12-16 18:41:53 | INFO | train_inner | epoch 002:    600 / 1102 loss=7.338, nll_loss=6.355, moe_gate_loss=1.04964, cmr_gate_loss=0, overflow_expert1=0.042, overflow_expert2=10.594, entropy_gating=0.967, expert1_balance_top=34.122, expert1_balance_bottom=15.172, unused_expert1_count=0, expert2_balance_top=33.537, expert2_balance_bottom=19.046, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=81.85, wps=16502.1, ups=4.59, wpb=3591.6, bsz=148.4, num_updates=1700, lr=0.0002125, gnorm=1.585, loss_scale=32, train_wall=21, gb_free=8.8, wall=637
2022-12-16 18:42:15 | INFO | train_inner | epoch 002:    700 / 1102 loss=7.176, nll_loss=6.167, moe_gate_loss=1.04697, cmr_gate_loss=0, overflow_expert1=0.009, overflow_expert2=11.104, entropy_gating=0.955, expert1_balance_top=34.073, expert1_balance_bottom=15.43, unused_expert1_count=0, expert2_balance_top=34.83, expert2_balance_bottom=18.233, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=71.87, wps=16268.6, ups=4.59, wpb=3547.1, bsz=149, num_updates=1800, lr=0.000225, gnorm=1.583, loss_scale=32, train_wall=21, gb_free=8.7, wall=658
2022-12-16 18:42:36 | INFO | train_inner | epoch 002:    800 / 1102 loss=7.05, nll_loss=6.021, moe_gate_loss=1.05377, cmr_gate_loss=0, overflow_expert1=0.025, overflow_expert2=11.73, entropy_gating=0.917, expert1_balance_top=34.218, expert1_balance_bottom=15.004, unused_expert1_count=0, expert2_balance_top=35.241, expert2_balance_bottom=17.495, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=64.96, wps=16615.8, ups=4.61, wpb=3607.1, bsz=146.7, num_updates=1900, lr=0.0002375, gnorm=1.538, loss_scale=32, train_wall=21, gb_free=8.7, wall=680
2022-12-16 18:42:58 | INFO | train_inner | epoch 002:    900 / 1102 loss=7.005, nll_loss=5.966, moe_gate_loss=1.06094, cmr_gate_loss=0, overflow_expert1=0.059, overflow_expert2=12.784, entropy_gating=0.888, expert1_balance_top=34.712, expert1_balance_bottom=15.073, unused_expert1_count=0, expert2_balance_top=36.016, expert2_balance_bottom=16.705, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=62.53, wps=16553.4, ups=4.6, wpb=3600.7, bsz=137.8, num_updates=2000, lr=0.00025, gnorm=1.501, loss_scale=32, train_wall=21, gb_free=8.9, wall=702
2022-12-16 18:43:20 | INFO | train_inner | epoch 002:   1000 / 1102 loss=6.855, nll_loss=5.795, moe_gate_loss=1.06925, cmr_gate_loss=0, overflow_expert1=0.099, overflow_expert2=14.117, entropy_gating=0.867, expert1_balance_top=35.144, expert1_balance_bottom=14.945, unused_expert1_count=0, expert2_balance_top=36.269, expert2_balance_bottom=16.009, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=55.52, wps=16109.6, ups=4.58, wpb=3515.5, bsz=141.3, num_updates=2100, lr=0.0002625, gnorm=1.482, loss_scale=32, train_wall=21, gb_free=8.9, wall=724
2022-12-16 18:43:42 | INFO | train_inner | epoch 002:   1100 / 1102 loss=6.78, nll_loss=5.706, moe_gate_loss=1.07007, cmr_gate_loss=0, overflow_expert1=0.104, overflow_expert2=14.571, entropy_gating=0.841, expert1_balance_top=34.489, expert1_balance_bottom=14.782, unused_expert1_count=0, expert2_balance_top=36.504, expert2_balance_bottom=15.62, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=52.22, wps=16541.7, ups=4.56, wpb=3624.9, bsz=140.6, num_updates=2200, lr=0.000275, gnorm=1.519, loss_scale=32, train_wall=22, gb_free=8.8, wall=746
2022-12-16 18:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 18:43:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:44:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:44:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:44:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:45:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.486 | nll_loss 5.325 | moe_gate_loss 1.0614 | cmr_gate_loss 0 | overflow_expert1 11.733 | overflow_expert2 88.377 | entropy_gating 0.831 | expert1_balance_top 34.715 | expert1_balance_bottom 17.221 | unused_expert1_count 0 | expert2_balance_top 37.471 | expert2_balance_bottom 15.257 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 40.09 | bleu 9.67 | wps 1324.4 | wpb 2835.3 | bsz 115.6 | num_updates 2202 | best_bleu 9.67
2022-12-16 18:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2202 updates
2022-12-16 18:45:58 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint2.pt
2022-12-16 18:45:59 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint2.pt
2022-12-16 18:46:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint2.pt (epoch 2 @ 2202 updates, score 9.67) (writing took 3.1596607000000176 seconds)
2022-12-16 18:46:01 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-12-16 18:46:01 | INFO | train | epoch 002 | loss 7.373 | nll_loss 6.392 | moe_gate_loss 1.05218 | cmr_gate_loss 0 | overflow_expert1 0.062 | overflow_expert2 11.252 | entropy_gating 1.01 | expert1_balance_top 33.952 | expert1_balance_bottom 15.32 | unused_expert1_count 0 | expert2_balance_top 33.89 | expert2_balance_bottom 18.317 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 84.01 | wps 10383.1 | ups 2.9 | wpb 3583.6 | bsz 145.4 | num_updates 2202 | lr 0.00027525 | gnorm 1.567 | loss_scale 32 | train_wall 237 | gb_free 8.8 | wall 885
2022-12-16 18:46:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 18:46:01 | INFO | fairseq.trainer | begin training epoch 3
2022-12-16 18:46:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 18:46:23 | INFO | train_inner | epoch 003:     98 / 1102 loss=6.543, nll_loss=5.439, moe_gate_loss=1.07014, cmr_gate_loss=0, overflow_expert1=0.054, overflow_expert2=15.364, entropy_gating=0.822, expert1_balance_top=34.218, expert1_balance_bottom=15.409, unused_expert1_count=0, expert2_balance_top=36.658, expert2_balance_bottom=15.453, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=43.38, wps=2232.6, ups=0.62, wpb=3604.3, bsz=140.9, num_updates=2300, lr=0.0002875, gnorm=1.402, loss_scale=32, train_wall=22, gb_free=8.8, wall=907
2022-12-16 18:46:45 | INFO | train_inner | epoch 003:    198 / 1102 loss=6.457, nll_loss=5.339, moe_gate_loss=1.08088, cmr_gate_loss=0, overflow_expert1=0.128, overflow_expert2=16.101, entropy_gating=0.808, expert1_balance_top=34.853, expert1_balance_bottom=14.609, unused_expert1_count=0, expert2_balance_top=36.772, expert2_balance_bottom=15.494, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=40.47, wps=16096.6, ups=4.53, wpb=3551.7, bsz=139.6, num_updates=2400, lr=0.0003, gnorm=1.414, loss_scale=32, train_wall=22, gb_free=8.8, wall=929
2022-12-16 18:47:08 | INFO | train_inner | epoch 003:    298 / 1102 loss=6.337, nll_loss=5.199, moe_gate_loss=1.08311, cmr_gate_loss=0, overflow_expert1=0.161, overflow_expert2=16.2, entropy_gating=0.783, expert1_balance_top=34.58, expert1_balance_bottom=14.608, unused_expert1_count=0, expert2_balance_top=36.905, expert2_balance_bottom=15.324, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=36.73, wps=16177.3, ups=4.5, wpb=3595.1, bsz=146.2, num_updates=2500, lr=0.0003125, gnorm=1.421, loss_scale=32, train_wall=22, gb_free=8.6, wall=951
2022-12-16 18:47:30 | INFO | train_inner | epoch 003:    398 / 1102 loss=6.239, nll_loss=5.083, moe_gate_loss=1.08577, cmr_gate_loss=0, overflow_expert1=0.146, overflow_expert2=16.607, entropy_gating=0.767, expert1_balance_top=34.486, expert1_balance_bottom=14.62, unused_expert1_count=0, expert2_balance_top=36.822, expert2_balance_bottom=15.092, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=33.9, wps=16374.7, ups=4.54, wpb=3607.5, bsz=141.5, num_updates=2600, lr=0.000325, gnorm=1.373, loss_scale=32, train_wall=22, gb_free=8.8, wall=973
2022-12-16 18:47:52 | INFO | train_inner | epoch 003:    498 / 1102 loss=6.206, nll_loss=5.044, moe_gate_loss=1.08242, cmr_gate_loss=0, overflow_expert1=0.166, overflow_expert2=16.981, entropy_gating=0.76, expert1_balance_top=34.078, expert1_balance_bottom=14.754, unused_expert1_count=0, expert2_balance_top=37.106, expert2_balance_bottom=15.05, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=32.98, wps=16112.2, ups=4.48, wpb=3594.3, bsz=134.9, num_updates=2700, lr=0.0003375, gnorm=1.352, loss_scale=32, train_wall=22, gb_free=8.7, wall=996
2022-12-16 18:48:14 | INFO | train_inner | epoch 003:    598 / 1102 loss=6.025, nll_loss=4.837, moe_gate_loss=1.08428, cmr_gate_loss=0, overflow_expert1=0.134, overflow_expert2=17.213, entropy_gating=0.746, expert1_balance_top=34.33, expert1_balance_bottom=14.82, unused_expert1_count=0, expert2_balance_top=37.349, expert2_balance_bottom=15.026, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=28.58, wps=15693.2, ups=4.46, wpb=3521.5, bsz=150.2, num_updates=2800, lr=0.00035, gnorm=1.391, loss_scale=32, train_wall=22, gb_free=8.9, wall=1018
2022-12-16 18:48:37 | INFO | train_inner | epoch 003:    698 / 1102 loss=5.873, nll_loss=4.66, moe_gate_loss=1.09206, cmr_gate_loss=0, overflow_expert1=0.18, overflow_expert2=17.72, entropy_gating=0.728, expert1_balance_top=34.623, expert1_balance_bottom=14.257, unused_expert1_count=0, expert2_balance_top=37.522, expert2_balance_bottom=14.846, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=25.29, wps=15960.4, ups=4.42, wpb=3611.9, bsz=159.4, num_updates=2900, lr=0.0003625, gnorm=1.321, loss_scale=32, train_wall=22, gb_free=8.6, wall=1041
2022-12-16 18:48:59 | INFO | train_inner | epoch 003:    798 / 1102 loss=5.813, nll_loss=4.591, moe_gate_loss=1.09074, cmr_gate_loss=0, overflow_expert1=0.105, overflow_expert2=18.57, entropy_gating=0.715, expert1_balance_top=34.259, expert1_balance_bottom=14.347, unused_expert1_count=0, expert2_balance_top=37.714, expert2_balance_bottom=14.823, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=24.09, wps=16105.2, ups=4.47, wpb=3601, bsz=154.4, num_updates=3000, lr=0.000375, gnorm=1.317, loss_scale=32, train_wall=22, gb_free=8.7, wall=1063
2022-12-16 18:49:22 | INFO | train_inner | epoch 003:    898 / 1102 loss=5.916, nll_loss=4.705, moe_gate_loss=1.08166, cmr_gate_loss=0, overflow_expert1=0.09, overflow_expert2=18.487, entropy_gating=0.712, expert1_balance_top=33.729, expert1_balance_bottom=14.786, unused_expert1_count=0, expert2_balance_top=37.809, expert2_balance_bottom=14.737, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=26.08, wps=15905.2, ups=4.49, wpb=3541.2, bsz=124, num_updates=3100, lr=0.0003875, gnorm=1.351, loss_scale=32, train_wall=22, gb_free=8.6, wall=1085
2022-12-16 18:49:44 | INFO | train_inner | epoch 003:    998 / 1102 loss=5.728, nll_loss=4.489, moe_gate_loss=1.09299, cmr_gate_loss=0, overflow_expert1=0.233, overflow_expert2=18.285, entropy_gating=0.697, expert1_balance_top=34.18, expert1_balance_bottom=14.354, unused_expert1_count=0, expert2_balance_top=37.66, expert2_balance_bottom=14.741, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=22.45, wps=16182.7, ups=4.45, wpb=3633.6, bsz=152.4, num_updates=3200, lr=0.0004, gnorm=1.334, loss_scale=32, train_wall=22, gb_free=8.6, wall=1108
2022-12-16 18:50:06 | INFO | train_inner | epoch 003:   1098 / 1102 loss=5.653, nll_loss=4.403, moe_gate_loss=1.10137, cmr_gate_loss=0, overflow_expert1=0.212, overflow_expert2=18.708, entropy_gating=0.686, expert1_balance_top=34.624, expert1_balance_bottom=13.737, unused_expert1_count=0, expert2_balance_top=37.59, expert2_balance_bottom=14.594, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=21.15, wps=15932.2, ups=4.49, wpb=3546.6, bsz=153.4, num_updates=3300, lr=0.0004125, gnorm=1.34, loss_scale=32, train_wall=22, gb_free=9.1, wall=1130
2022-12-16 18:50:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 18:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:51:40 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.639 | nll_loss 4.314 | moe_gate_loss 1.10288 | cmr_gate_loss 0 | overflow_expert1 14.324 | overflow_expert2 87.456 | entropy_gating 0.679 | expert1_balance_top 35.265 | expert1_balance_bottom 13.564 | unused_expert1_count 0 | expert2_balance_top 36.404 | expert2_balance_bottom 16.021 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 19.89 | bleu 13.5 | wps 1923.6 | wpb 2835.3 | bsz 115.6 | num_updates 3304 | best_bleu 13.5
2022-12-16 18:51:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3304 updates
2022-12-16 18:51:40 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint3.pt
2022-12-16 18:51:42 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint3.pt
2022-12-16 18:51:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint3.pt (epoch 3 @ 3304 updates, score 13.5) (writing took 3.2840300999998817 seconds)
2022-12-16 18:51:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-12-16 18:51:43 | INFO | train | epoch 003 | loss 6.068 | nll_loss 4.885 | moe_gate_loss 1.08604 | cmr_gate_loss 0 | overflow_expert1 0.146 | overflow_expert2 17.295 | entropy_gating 0.679 | expert1_balance_top 34.358 | expert1_balance_bottom 14.568 | unused_expert1_count 0 | expert2_balance_top 37.265 | expert2_balance_bottom 15.017 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 29.55 | wps 11541.8 | ups 3.22 | wpb 3583.6 | bsz 145.4 | num_updates 3304 | lr 0.000413 | gnorm 1.365 | loss_scale 32 | train_wall 241 | gb_free 8.6 | wall 1227
2022-12-16 18:51:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 18:51:44 | INFO | fairseq.trainer | begin training epoch 4
2022-12-16 18:51:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 18:52:05 | INFO | train_inner | epoch 004:     96 / 1102 loss=5.457, nll_loss=4.178, moe_gate_loss=1.09873, cmr_gate_loss=0, overflow_expert1=0.192, overflow_expert2=18.47, entropy_gating=0.672, expert1_balance_top=34.323, expert1_balance_bottom=13.926, unused_expert1_count=0, expert2_balance_top=37.497, expert2_balance_bottom=14.714, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=18.1, wps=3023.8, ups=0.84, wpb=3583.7, bsz=150.9, num_updates=3400, lr=0.000425, gnorm=1.292, loss_scale=32, train_wall=22, gb_free=9, wall=1249
2022-12-16 18:52:27 | INFO | train_inner | epoch 004:    196 / 1102 loss=5.449, nll_loss=4.166, moe_gate_loss=1.09299, cmr_gate_loss=0, overflow_expert1=0.082, overflow_expert2=19.384, entropy_gating=0.674, expert1_balance_top=33.522, expert1_balance_bottom=14.304, unused_expert1_count=0, expert2_balance_top=37.696, expert2_balance_bottom=14.366, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=17.96, wps=16236.6, ups=4.58, wpb=3545.2, bsz=137.6, num_updates=3500, lr=0.0004375, gnorm=1.276, loss_scale=32, train_wall=22, gb_free=8.8, wall=1270
2022-12-16 18:52:49 | INFO | train_inner | epoch 004:    296 / 1102 loss=5.395, nll_loss=4.102, moe_gate_loss=1.08615, cmr_gate_loss=0, overflow_expert1=0.026, overflow_expert2=18.45, entropy_gating=0.658, expert1_balance_top=33.227, expert1_balance_bottom=14.302, unused_expert1_count=0, expert2_balance_top=37.867, expert2_balance_bottom=14.498, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=17.17, wps=16499.3, ups=4.52, wpb=3647.8, bsz=131.3, num_updates=3600, lr=0.00045, gnorm=1.248, loss_scale=32, train_wall=22, gb_free=8.8, wall=1293
2022-12-16 18:53:10 | INFO | train_inner | epoch 004:    396 / 1102 loss=5.396, nll_loss=4.101, moe_gate_loss=1.09233, cmr_gate_loss=0, overflow_expert1=0.051, overflow_expert2=19.45, entropy_gating=0.661, expert1_balance_top=33.708, expert1_balance_bottom=14.195, unused_expert1_count=0, expert2_balance_top=37.637, expert2_balance_bottom=14.292, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=17.16, wps=16225.1, ups=4.64, wpb=3496.7, bsz=136.1, num_updates=3700, lr=0.0004625, gnorm=1.257, loss_scale=32, train_wall=21, gb_free=8.9, wall=1314
2022-12-16 18:53:32 | INFO | train_inner | epoch 004:    496 / 1102 loss=5.328, nll_loss=4.021, moe_gate_loss=1.09459, cmr_gate_loss=0, overflow_expert1=0.005, overflow_expert2=20.008, entropy_gating=0.645, expert1_balance_top=33.622, expert1_balance_bottom=13.956, unused_expert1_count=0, expert2_balance_top=37.866, expert2_balance_bottom=14.237, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=16.24, wps=16422.9, ups=4.54, wpb=3618.5, bsz=138.7, num_updates=3800, lr=0.000475, gnorm=1.229, loss_scale=32, train_wall=22, gb_free=8.8, wall=1336
2022-12-16 18:53:54 | INFO | train_inner | epoch 004:    596 / 1102 loss=5.282, nll_loss=3.97, moe_gate_loss=1.0938, cmr_gate_loss=0, overflow_expert1=0.068, overflow_expert2=19.265, entropy_gating=0.641, expert1_balance_top=33.751, expert1_balance_bottom=14.221, unused_expert1_count=0, expert2_balance_top=37.973, expert2_balance_bottom=14.335, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=15.67, wps=16369.9, ups=4.57, wpb=3581.8, bsz=142.1, num_updates=3900, lr=0.0004875, gnorm=1.217, loss_scale=32, train_wall=22, gb_free=8.9, wall=1358
2022-12-16 18:54:17 | INFO | train_inner | epoch 004:    696 / 1102 loss=5.232, nll_loss=3.911, moe_gate_loss=1.09554, cmr_gate_loss=0, overflow_expert1=0.023, overflow_expert2=19.745, entropy_gating=0.641, expert1_balance_top=33.839, expert1_balance_bottom=13.868, unused_expert1_count=0, expert2_balance_top=38.336, expert2_balance_bottom=14.274, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=15.05, wps=16205.7, ups=4.49, wpb=3612.9, bsz=145, num_updates=4000, lr=0.0005, gnorm=1.198, loss_scale=32, train_wall=22, gb_free=8.9, wall=1380
2022-12-16 18:54:39 | INFO | train_inner | epoch 004:    796 / 1102 loss=5.158, nll_loss=3.827, moe_gate_loss=1.09685, cmr_gate_loss=0, overflow_expert1=0.075, overflow_expert2=19.484, entropy_gating=0.647, expert1_balance_top=34.114, expert1_balance_bottom=14.045, unused_expert1_count=0, expert2_balance_top=38.285, expert2_balance_bottom=14.074, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=14.19, wps=16110.9, ups=4.56, wpb=3531.8, bsz=161.1, num_updates=4100, lr=0.000493865, gnorm=1.269, loss_scale=32, train_wall=22, gb_free=8.7, wall=1402
2022-12-16 18:55:01 | INFO | train_inner | epoch 004:    896 / 1102 loss=5.174, nll_loss=3.843, moe_gate_loss=1.09765, cmr_gate_loss=0, overflow_expert1=0.088, overflow_expert2=19.843, entropy_gating=0.639, expert1_balance_top=33.893, expert1_balance_bottom=13.724, unused_expert1_count=0.002, expert2_balance_top=37.93, expert2_balance_bottom=14.346, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=14.35, wps=16198.9, ups=4.53, wpb=3574.2, bsz=152.9, num_updates=4200, lr=0.00048795, gnorm=1.246, loss_scale=32, train_wall=22, gb_free=8.8, wall=1424
2022-12-16 18:55:23 | INFO | train_inner | epoch 004:    996 / 1102 loss=5.098, nll_loss=3.756, moe_gate_loss=1.09494, cmr_gate_loss=0, overflow_expert1=0.047, overflow_expert2=19.584, entropy_gating=0.619, expert1_balance_top=33.368, expert1_balance_bottom=13.831, unused_expert1_count=0, expert2_balance_top=37.973, expert2_balance_bottom=14.478, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=13.51, wps=16561.9, ups=4.54, wpb=3649.1, bsz=144.9, num_updates=4300, lr=0.000482243, gnorm=1.135, loss_scale=32, train_wall=22, gb_free=9, wall=1446
2022-12-16 18:55:45 | INFO | train_inner | epoch 004:   1096 / 1102 loss=4.984, nll_loss=3.629, moe_gate_loss=1.09196, cmr_gate_loss=0, overflow_expert1=0.012, overflow_expert2=19.295, entropy_gating=0.62, expert1_balance_top=33.638, expert1_balance_bottom=14.284, unused_expert1_count=0, expert2_balance_top=38.023, expert2_balance_bottom=14.508, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=12.38, wps=16238.1, ups=4.53, wpb=3586.2, bsz=160.7, num_updates=4400, lr=0.000476731, gnorm=1.134, loss_scale=32, train_wall=22, gb_free=9, wall=1468
2022-12-16 18:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 18:55:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:55:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:55:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:55:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 18:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 18:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 18:57:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.943 | nll_loss 3.491 | moe_gate_loss 1.08694 | cmr_gate_loss 0 | overflow_expert1 11.905 | overflow_expert2 90.283 | entropy_gating 0.62 | expert1_balance_top 34.325 | expert1_balance_bottom 15.658 | unused_expert1_count 0 | expert2_balance_top 39.031 | expert2_balance_bottom 14.04 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 11.24 | bleu 23.75 | wps 1793.6 | wpb 2835.3 | bsz 115.6 | num_updates 4406 | best_bleu 23.75
2022-12-16 18:57:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4406 updates
2022-12-16 18:57:25 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint4.pt
2022-12-16 18:57:27 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint4.pt
2022-12-16 18:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint4.pt (epoch 4 @ 4406 updates, score 23.75) (writing took 3.3636668999999983 seconds)
2022-12-16 18:57:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-12-16 18:57:29 | INFO | train | epoch 004 | loss 5.267 | nll_loss 3.953 | moe_gate_loss 1.09411 | cmr_gate_loss 0 | overflow_expert1 0.061 | overflow_expert2 19.372 | entropy_gating 0.625 | expert1_balance_top 33.729 | expert1_balance_bottom 14.063 | unused_expert1_count 0 | expert2_balance_top 37.919 | expert2_balance_bottom 14.374 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 15.49 | wps 11434.9 | ups 3.19 | wpb 3583.6 | bsz 145.4 | num_updates 4406 | lr 0.000476407 | gnorm 1.227 | loss_scale 32 | train_wall 239 | gb_free 8.7 | wall 1573
2022-12-16 18:57:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 18:57:29 | INFO | fairseq.trainer | begin training epoch 5
2022-12-16 18:57:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 18:57:51 | INFO | train_inner | epoch 005:     94 / 1102 loss=4.859, nll_loss=3.486, moe_gate_loss=1.09173, cmr_gate_loss=0, overflow_expert1=0.028, overflow_expert2=19.712, entropy_gating=0.622, expert1_balance_top=33.527, expert1_balance_bottom=14.484, unused_expert1_count=0.002, expert2_balance_top=37.985, expert2_balance_bottom=14.202, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=11.2, wps=2838.1, ups=0.79, wpb=3593.8, bsz=151.7, num_updates=4500, lr=0.000471405, gnorm=1.105, loss_scale=32, train_wall=23, gb_free=8.9, wall=1595
2022-12-16 18:58:14 | INFO | train_inner | epoch 005:    194 / 1102 loss=4.776, nll_loss=3.39, moe_gate_loss=1.08875, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.594, entropy_gating=0.611, expert1_balance_top=33.349, expert1_balance_bottom=14.465, unused_expert1_count=0, expert2_balance_top=38.069, expert2_balance_bottom=14.225, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=10.49, wps=15948, ups=4.42, wpb=3607.7, bsz=152.5, num_updates=4600, lr=0.000466252, gnorm=1.058, loss_scale=32, train_wall=22, gb_free=8.7, wall=1618
2022-12-16 18:58:37 | INFO | train_inner | epoch 005:    294 / 1102 loss=4.812, nll_loss=3.431, moe_gate_loss=1.0904, cmr_gate_loss=0, overflow_expert1=0.009, overflow_expert2=19.62, entropy_gating=0.612, expert1_balance_top=33.125, expert1_balance_bottom=14.377, unused_expert1_count=0, expert2_balance_top=38.227, expert2_balance_bottom=13.982, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=10.79, wps=15495.5, ups=4.29, wpb=3609.2, bsz=144.2, num_updates=4700, lr=0.000461266, gnorm=1.083, loss_scale=32, train_wall=23, gb_free=9, wall=1641
2022-12-16 18:59:01 | INFO | train_inner | epoch 005:    394 / 1102 loss=4.757, nll_loss=3.369, moe_gate_loss=1.08868, cmr_gate_loss=0, overflow_expert1=0.013, overflow_expert2=19.577, entropy_gating=0.612, expert1_balance_top=33.215, expert1_balance_bottom=14.551, unused_expert1_count=0, expert2_balance_top=38.231, expert2_balance_bottom=14.169, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=10.33, wps=15185.9, ups=4.3, wpb=3535.2, bsz=143.8, num_updates=4800, lr=0.000456435, gnorm=1.049, loss_scale=32, train_wall=23, gb_free=9, wall=1664
2022-12-16 18:59:23 | INFO | train_inner | epoch 005:    494 / 1102 loss=4.851, nll_loss=3.474, moe_gate_loss=1.08466, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.685, entropy_gating=0.608, expert1_balance_top=32.67, expert1_balance_bottom=14.737, unused_expert1_count=0, expert2_balance_top=38.694, expert2_balance_bottom=13.973, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=11.11, wps=15535.1, ups=4.41, wpb=3523.9, bsz=126.9, num_updates=4900, lr=0.000451754, gnorm=1.137, loss_scale=32, train_wall=22, gb_free=8.7, wall=1687
2022-12-16 18:59:46 | INFO | train_inner | epoch 005:    594 / 1102 loss=4.828, nll_loss=3.449, moe_gate_loss=1.08664, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.504, entropy_gating=0.615, expert1_balance_top=32.68, expert1_balance_bottom=14.559, unused_expert1_count=0, expert2_balance_top=38.559, expert2_balance_bottom=13.846, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=10.92, wps=15323.8, ups=4.36, wpb=3517.5, bsz=133.8, num_updates=5000, lr=0.000447214, gnorm=1.113, loss_scale=32, train_wall=23, gb_free=8.9, wall=1710
2022-12-16 19:00:09 | INFO | train_inner | epoch 005:    694 / 1102 loss=4.607, nll_loss=3.198, moe_gate_loss=1.09024, cmr_gate_loss=0, overflow_expert1=0.001, overflow_expert2=20.505, entropy_gating=0.605, expert1_balance_top=33.19, expert1_balance_bottom=14.325, unused_expert1_count=0, expert2_balance_top=38.803, expert2_balance_bottom=13.886, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=9.17, wps=16164, ups=4.38, wpb=3689.6, bsz=164.4, num_updates=5100, lr=0.000442807, gnorm=1.003, loss_scale=32, train_wall=22, gb_free=8.7, wall=1733
2022-12-16 19:00:32 | INFO | train_inner | epoch 005:    794 / 1102 loss=4.717, nll_loss=3.323, moe_gate_loss=1.08453, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.031, entropy_gating=0.606, expert1_balance_top=32.802, expert1_balance_bottom=14.768, unused_expert1_count=0, expert2_balance_top=38.852, expert2_balance_bottom=13.926, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=10, wps=15856, ups=4.43, wpb=3580.1, bsz=140.3, num_updates=5200, lr=0.000438529, gnorm=1.027, loss_scale=32, train_wall=22, gb_free=8.7, wall=1755
2022-12-16 19:00:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-16 19:00:55 | INFO | train_inner | epoch 005:    895 / 1102 loss=4.685, nll_loss=3.286, moe_gate_loss=1.0867, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.596, entropy_gating=0.616, expert1_balance_top=33.057, expert1_balance_bottom=14.604, unused_expert1_count=0, expert2_balance_top=39.043, expert2_balance_bottom=13.713, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=9.76, wps=15096.8, ups=4.26, wpb=3544.7, bsz=150.2, num_updates=5300, lr=0.000434372, gnorm=1.111, loss_scale=16, train_wall=23, gb_free=8.7, wall=1779
2022-12-16 19:01:18 | INFO | train_inner | epoch 005:    995 / 1102 loss=4.689, nll_loss=3.291, moe_gate_loss=1.08726, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.739, entropy_gating=0.601, expert1_balance_top=33.011, expert1_balance_bottom=14.347, unused_expert1_count=0, expert2_balance_top=39.591, expert2_balance_bottom=13.73, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=9.79, wps=15669, ups=4.38, wpb=3576.8, bsz=136.7, num_updates=5400, lr=0.000430331, gnorm=1.033, loss_scale=16, train_wall=22, gb_free=8.9, wall=1802
2022-12-16 19:01:41 | INFO | train_inner | epoch 005:   1095 / 1102 loss=4.617, nll_loss=3.211, moe_gate_loss=1.08891, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=21.28, entropy_gating=0.601, expert1_balance_top=33.011, expert1_balance_bottom=14.4, unused_expert1_count=0, expert2_balance_top=39.302, expert2_balance_bottom=13.594, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=9.26, wps=15422.3, ups=4.24, wpb=3635.5, bsz=151.7, num_updates=5500, lr=0.000426401, gnorm=1.029, loss_scale=16, train_wall=23, gb_free=8.6, wall=1825
2022-12-16 19:01:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 19:01:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:02:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:02:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:02:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:03:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.634 | nll_loss 3.136 | moe_gate_loss 1.07189 | cmr_gate_loss 0 | overflow_expert1 10.864 | overflow_expert2 91.887 | entropy_gating 0.614 | expert1_balance_top 31.829 | expert1_balance_bottom 15.803 | unused_expert1_count 0 | expert2_balance_top 38.837 | expert2_balance_bottom 13.59 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 8.79 | bleu 27.29 | wps 1990 | wpb 2835.3 | bsz 115.6 | num_updates 5507 | best_bleu 27.29
2022-12-16 19:03:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5507 updates
2022-12-16 19:03:13 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint5.pt
2022-12-16 19:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint5.pt
2022-12-16 19:03:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint5.pt (epoch 5 @ 5507 updates, score 27.29) (writing took 3.1691275000000587 seconds)
2022-12-16 19:03:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-12-16 19:03:16 | INFO | train | epoch 005 | loss 4.742 | nll_loss 3.351 | moe_gate_loss 1.08797 | cmr_gate_loss 0 | overflow_expert1 0.005 | overflow_expert2 20.159 | entropy_gating 0.566 | expert1_balance_top 33.047 | expert1_balance_bottom 14.511 | unused_expert1_count 0 | expert2_balance_top 38.675 | expert2_balance_bottom 13.924 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 10.21 | wps 11363.3 | ups 3.17 | wpb 3583.4 | bsz 145 | num_updates 5507 | lr 0.00042613 | gnorm 1.067 | loss_scale 16 | train_wall 249 | gb_free 9.1 | wall 1920
2022-12-16 19:03:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 19:03:16 | INFO | fairseq.trainer | begin training epoch 6
2022-12-16 19:03:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 19:03:36 | INFO | train_inner | epoch 006:     93 / 1102 loss=4.475, nll_loss=3.049, moe_gate_loss=1.08411, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.752, entropy_gating=0.608, expert1_balance_top=32.556, expert1_balance_bottom=14.654, unused_expert1_count=0, expert2_balance_top=38.929, expert2_balance_bottom=13.699, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=8.28, wps=3065, ups=0.87, wpb=3515, bsz=140.7, num_updates=5600, lr=0.000422577, gnorm=1.022, loss_scale=16, train_wall=21, gb_free=8.7, wall=1940
2022-12-16 19:03:58 | INFO | train_inner | epoch 006:    193 / 1102 loss=4.455, nll_loss=3.024, moe_gate_loss=1.08859, cmr_gate_loss=0, overflow_expert1=0.003, overflow_expert2=20.542, entropy_gating=0.596, expert1_balance_top=33.147, expert1_balance_bottom=14.583, unused_expert1_count=0, expert2_balance_top=38.827, expert2_balance_bottom=13.712, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=8.14, wps=16430.4, ups=4.64, wpb=3543, bsz=152.9, num_updates=5700, lr=0.000418854, gnorm=1.023, loss_scale=16, train_wall=21, gb_free=8.8, wall=1961
2022-12-16 19:04:19 | INFO | train_inner | epoch 006:    293 / 1102 loss=4.445, nll_loss=3.012, moe_gate_loss=1.08402, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.12, entropy_gating=0.593, expert1_balance_top=32.54, expert1_balance_bottom=14.939, unused_expert1_count=0, expert2_balance_top=38.842, expert2_balance_bottom=13.935, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=8.07, wps=16218.1, ups=4.61, wpb=3521.3, bsz=137.2, num_updates=5800, lr=0.000415227, gnorm=0.997, loss_scale=16, train_wall=21, gb_free=9.1, wall=1983
2022-12-16 19:04:42 | INFO | train_inner | epoch 006:    393 / 1102 loss=4.377, nll_loss=2.935, moe_gate_loss=1.08701, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.257, entropy_gating=0.586, expert1_balance_top=32.792, expert1_balance_bottom=14.452, unused_expert1_count=0, expert2_balance_top=39.051, expert2_balance_bottom=13.675, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.65, wps=15786.1, ups=4.34, wpb=3640.9, bsz=152, num_updates=5900, lr=0.000411693, gnorm=0.955, loss_scale=16, train_wall=23, gb_free=8.7, wall=2006
2022-12-16 19:05:06 | INFO | train_inner | epoch 006:    493 / 1102 loss=4.403, nll_loss=2.964, moe_gate_loss=1.08186, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.492, entropy_gating=0.582, expert1_balance_top=32.419, expert1_balance_bottom=14.861, unused_expert1_count=0, expert2_balance_top=39.228, expert2_balance_bottom=13.896, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.8, wps=15325.6, ups=4.19, wpb=3653.7, bsz=143.1, num_updates=6000, lr=0.000408248, gnorm=0.944, loss_scale=16, train_wall=23, gb_free=8.9, wall=2030
2022-12-16 19:05:29 | INFO | train_inner | epoch 006:    593 / 1102 loss=4.48, nll_loss=3.052, moe_gate_loss=1.08435, cmr_gate_loss=0, overflow_expert1=0.004, overflow_expert2=19.914, entropy_gating=0.588, expert1_balance_top=32.408, expert1_balance_bottom=14.734, unused_expert1_count=0, expert2_balance_top=38.576, expert2_balance_bottom=14.014, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=8.3, wps=15302.2, ups=4.33, wpb=3531.4, bsz=131.8, num_updates=6100, lr=0.000404888, gnorm=1, loss_scale=16, train_wall=23, gb_free=9.6, wall=2053
2022-12-16 19:05:53 | INFO | train_inner | epoch 006:    693 / 1102 loss=4.381, nll_loss=2.941, moe_gate_loss=1.08812, cmr_gate_loss=0, overflow_expert1=0.003, overflow_expert2=20.132, entropy_gating=0.589, expert1_balance_top=32.929, expert1_balance_bottom=14.503, unused_expert1_count=0, expert2_balance_top=38.884, expert2_balance_bottom=14.19, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.68, wps=15231.3, ups=4.26, wpb=3574.2, bsz=158.6, num_updates=6200, lr=0.00040161, gnorm=0.979, loss_scale=16, train_wall=23, gb_free=9.2, wall=2077
2022-12-16 19:06:17 | INFO | train_inner | epoch 006:    793 / 1102 loss=4.297, nll_loss=2.846, moe_gate_loss=1.08787, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.394, entropy_gating=0.58, expert1_balance_top=32.709, expert1_balance_bottom=14.528, unused_expert1_count=0, expert2_balance_top=38.832, expert2_balance_bottom=13.993, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.19, wps=15379.2, ups=4.17, wpb=3683.8, bsz=163.9, num_updates=6300, lr=0.00039841, gnorm=0.911, loss_scale=16, train_wall=24, gb_free=8.8, wall=2101
2022-12-16 19:06:40 | INFO | train_inner | epoch 006:    893 / 1102 loss=4.449, nll_loss=3.019, moe_gate_loss=1.08471, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.989, entropy_gating=0.592, expert1_balance_top=32.617, expert1_balance_bottom=14.736, unused_expert1_count=0, expert2_balance_top=38.856, expert2_balance_bottom=13.754, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=8.1, wps=15457.9, ups=4.34, wpb=3562.1, bsz=134.7, num_updates=6400, lr=0.000395285, gnorm=0.974, loss_scale=16, train_wall=23, gb_free=8.7, wall=2124
2022-12-16 19:07:03 | INFO | train_inner | epoch 006:    993 / 1102 loss=4.423, nll_loss=2.99, moe_gate_loss=1.08049, cmr_gate_loss=0, overflow_expert1=0.011, overflow_expert2=20.6, entropy_gating=0.589, expert1_balance_top=32.335, expert1_balance_bottom=15.051, unused_expert1_count=0, expert2_balance_top=38.805, expert2_balance_bottom=13.602, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.94, wps=15292.2, ups=4.27, wpb=3579.7, bsz=134.5, num_updates=6500, lr=0.000392232, gnorm=0.968, loss_scale=16, train_wall=23, gb_free=8.9, wall=2147
2022-12-16 19:07:27 | INFO | train_inner | epoch 006:   1093 / 1102 loss=4.355, nll_loss=2.912, moe_gate_loss=1.08498, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.521, entropy_gating=0.591, expert1_balance_top=32.412, expert1_balance_bottom=14.586, unused_expert1_count=0, expert2_balance_top=38.971, expert2_balance_bottom=13.618, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.53, wps=15344, ups=4.26, wpb=3600.1, bsz=147.2, num_updates=6600, lr=0.000389249, gnorm=0.948, loss_scale=16, train_wall=23, gb_free=9.2, wall=2170
2022-12-16 19:07:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 19:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:08:57 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.474 | nll_loss 2.948 | moe_gate_loss 1.07084 | cmr_gate_loss 0 | overflow_expert1 9.851 | overflow_expert2 92.777 | entropy_gating 0.605 | expert1_balance_top 31.168 | expert1_balance_bottom 16.422 | unused_expert1_count 0 | expert2_balance_top 38.678 | expert2_balance_bottom 13.37 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 7.72 | bleu 24.24 | wps 2020.1 | wpb 2835.3 | bsz 115.6 | num_updates 6609 | best_bleu 27.29
2022-12-16 19:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6609 updates
2022-12-16 19:08:57 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint6.pt
2022-12-16 19:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint6.pt
2022-12-16 19:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint6.pt (epoch 6 @ 6609 updates, score 24.24) (writing took 2.3445803999998134 seconds)
2022-12-16 19:09:00 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-12-16 19:09:00 | INFO | train | epoch 006 | loss 4.409 | nll_loss 2.973 | moe_gate_loss 1.08508 | cmr_gate_loss 0 | overflow_expert1 0.002 | overflow_expert2 20.349 | entropy_gating 0.555 | expert1_balance_top 32.618 | expert1_balance_bottom 14.697 | unused_expert1_count 0 | expert2_balance_top 38.893 | expert2_balance_bottom 13.826 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 7.85 | wps 11492 | ups 3.21 | wpb 3583.6 | bsz 145.4 | num_updates 6609 | lr 0.000388984 | gnorm 0.974 | loss_scale 16 | train_wall 249 | gb_free 8.7 | wall 2263
2022-12-16 19:09:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 19:09:00 | INFO | fairseq.trainer | begin training epoch 7
2022-12-16 19:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 19:09:21 | INFO | train_inner | epoch 007:     91 / 1102 loss=4.16, nll_loss=2.691, moe_gate_loss=1.08429, cmr_gate_loss=0, overflow_expert1=0.001, overflow_expert2=20.504, entropy_gating=0.588, expert1_balance_top=32.313, expert1_balance_bottom=14.771, unused_expert1_count=0, expert2_balance_top=38.905, expert2_balance_bottom=13.724, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.46, wps=3127.7, ups=0.87, wpb=3578.3, bsz=148.4, num_updates=6700, lr=0.000386334, gnorm=0.905, loss_scale=16, train_wall=23, gb_free=8.7, wall=2285
2022-12-16 19:09:45 | INFO | train_inner | epoch 007:    191 / 1102 loss=4.129, nll_loss=2.655, moe_gate_loss=1.08753, cmr_gate_loss=0, overflow_expert1=0.012, overflow_expert2=20.078, entropy_gating=0.585, expert1_balance_top=32.666, expert1_balance_bottom=14.665, unused_expert1_count=0, expert2_balance_top=38.855, expert2_balance_bottom=13.839, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.3, wps=15375.8, ups=4.26, wpb=3607.8, bsz=160.2, num_updates=6800, lr=0.000383482, gnorm=0.9, loss_scale=16, train_wall=23, gb_free=9.1, wall=2308
2022-12-16 19:10:08 | INFO | train_inner | epoch 007:    291 / 1102 loss=4.182, nll_loss=2.712, moe_gate_loss=1.08657, cmr_gate_loss=0, overflow_expert1=0.005, overflow_expert2=20.26, entropy_gating=0.58, expert1_balance_top=32.576, expert1_balance_bottom=14.535, unused_expert1_count=0, expert2_balance_top=38.649, expert2_balance_bottom=13.668, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.55, wps=15385, ups=4.23, wpb=3638.6, bsz=148.6, num_updates=6900, lr=0.000380693, gnorm=0.933, loss_scale=16, train_wall=23, gb_free=8.8, wall=2332
2022-12-16 19:10:32 | INFO | train_inner | epoch 007:    391 / 1102 loss=4.202, nll_loss=2.737, moe_gate_loss=1.08342, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.012, entropy_gating=0.579, expert1_balance_top=32.408, expert1_balance_bottom=14.844, unused_expert1_count=0, expert2_balance_top=38.591, expert2_balance_bottom=13.655, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.67, wps=15495.6, ups=4.26, wpb=3636.8, bsz=137.5, num_updates=7000, lr=0.000377964, gnorm=0.917, loss_scale=16, train_wall=23, gb_free=8.8, wall=2355
2022-12-16 19:10:55 | INFO | train_inner | epoch 007:    491 / 1102 loss=4.233, nll_loss=2.773, moe_gate_loss=1.08141, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.288, entropy_gating=0.589, expert1_balance_top=32.253, expert1_balance_bottom=14.946, unused_expert1_count=0.002, expert2_balance_top=38.351, expert2_balance_bottom=13.76, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.83, wps=15029.1, ups=4.32, wpb=3480.7, bsz=140.1, num_updates=7100, lr=0.000375293, gnorm=0.98, loss_scale=16, train_wall=23, gb_free=8.9, wall=2379
2022-12-16 19:11:18 | INFO | train_inner | epoch 007:    591 / 1102 loss=4.229, nll_loss=2.766, moe_gate_loss=1.08387, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.15, entropy_gating=0.581, expert1_balance_top=32.4, expert1_balance_bottom=14.749, unused_expert1_count=0.002, expert2_balance_top=38.857, expert2_balance_bottom=13.668, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.8, wps=15351.9, ups=4.29, wpb=3574.5, bsz=139.8, num_updates=7200, lr=0.000372678, gnorm=0.951, loss_scale=16, train_wall=23, gb_free=8.8, wall=2402
2022-12-16 19:11:41 | INFO | train_inner | epoch 007:    691 / 1102 loss=4.271, nll_loss=2.815, moe_gate_loss=1.08408, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.917, entropy_gating=0.584, expert1_balance_top=32.573, expert1_balance_bottom=14.673, unused_expert1_count=0, expert2_balance_top=39.037, expert2_balance_bottom=13.574, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=7.04, wps=15337.1, ups=4.35, wpb=3522.4, bsz=139.3, num_updates=7300, lr=0.000370117, gnorm=0.988, loss_scale=16, train_wall=23, gb_free=8.7, wall=2425
2022-12-16 19:12:04 | INFO | train_inner | epoch 007:    791 / 1102 loss=4.169, nll_loss=2.699, moe_gate_loss=1.08519, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.96, entropy_gating=0.581, expert1_balance_top=32.618, expert1_balance_bottom=14.493, unused_expert1_count=0, expert2_balance_top=39.028, expert2_balance_bottom=13.805, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.49, wps=15599.3, ups=4.3, wpb=3631.1, bsz=147.4, num_updates=7400, lr=0.000367607, gnorm=0.905, loss_scale=16, train_wall=23, gb_free=9, wall=2448
2022-12-16 19:12:28 | INFO | train_inner | epoch 007:    891 / 1102 loss=4.187, nll_loss=2.722, moe_gate_loss=1.08411, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.921, entropy_gating=0.583, expert1_balance_top=32.385, expert1_balance_bottom=14.534, unused_expert1_count=0, expert2_balance_top=38.773, expert2_balance_bottom=14.167, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.6, wps=15181.8, ups=4.29, wpb=3540.8, bsz=155.5, num_updates=7500, lr=0.000365148, gnorm=0.932, loss_scale=16, train_wall=23, gb_free=8.8, wall=2471
2022-12-16 19:12:51 | INFO | train_inner | epoch 007:    991 / 1102 loss=4.195, nll_loss=2.73, moe_gate_loss=1.0779, cmr_gate_loss=0, overflow_expert1=0.002, overflow_expert2=19.917, entropy_gating=0.584, expert1_balance_top=32.169, expert1_balance_bottom=15.159, unused_expert1_count=0, expert2_balance_top=39.027, expert2_balance_bottom=13.938, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.63, wps=15417.2, ups=4.25, wpb=3631.4, bsz=143.4, num_updates=7600, lr=0.000362738, gnorm=0.913, loss_scale=16, train_wall=23, gb_free=8.9, wall=2495
2022-12-16 19:13:15 | INFO | train_inner | epoch 007:   1091 / 1102 loss=4.192, nll_loss=2.727, moe_gate_loss=1.07785, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.398, entropy_gating=0.588, expert1_balance_top=32.194, expert1_balance_bottom=15.111, unused_expert1_count=0, expert2_balance_top=39.432, expert2_balance_bottom=13.385, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.62, wps=15174, ups=4.23, wpb=3589.7, bsz=137.4, num_updates=7700, lr=0.000360375, gnorm=0.908, loss_scale=16, train_wall=23, gb_free=8.8, wall=2519
2022-12-16 19:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 19:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:14:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.362 | nll_loss 2.822 | moe_gate_loss 1.07389 | cmr_gate_loss 0 | overflow_expert1 10.968 | overflow_expert2 91.237 | entropy_gating 0.61 | expert1_balance_top 32.119 | expert1_balance_bottom 15.834 | unused_expert1_count 0 | expert2_balance_top 38.389 | expert2_balance_bottom 13.578 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 7.07 | bleu 31.51 | wps 1831.7 | wpb 2835.3 | bsz 115.6 | num_updates 7711 | best_bleu 31.51
2022-12-16 19:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7711 updates
2022-12-16 19:14:55 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint7.pt
2022-12-16 19:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint7.pt
2022-12-16 19:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint7.pt (epoch 7 @ 7711 updates, score 31.51) (writing took 3.363566699999865 seconds)
2022-12-16 19:14:58 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-12-16 19:14:58 | INFO | train | epoch 007 | loss 4.194 | nll_loss 2.728 | moe_gate_loss 1.08338 | cmr_gate_loss 0 | overflow_expert1 0.002 | overflow_expert2 20.22 | entropy_gating 0.546 | expert1_balance_top 32.425 | expert1_balance_bottom 14.768 | unused_expert1_count 0 | expert2_balance_top 38.859 | expert2_balance_bottom 13.749 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0 | ppl 6.62 | wps 11010.6 | ups 3.07 | wpb 3583.6 | bsz 145.4 | num_updates 7711 | lr 0.000360118 | gnorm 0.93 | loss_scale 16 | train_wall 253 | gb_free 8.7 | wall 2622
2022-12-16 19:14:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 19:14:58 | INFO | fairseq.trainer | begin training epoch 8
2022-12-16 19:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 19:15:19 | INFO | train_inner | epoch 008:     89 / 1102 loss=4.005, nll_loss=2.515, moe_gate_loss=1.08148, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.93, entropy_gating=0.589, expert1_balance_top=32.438, expert1_balance_bottom=14.968, unused_expert1_count=0, expert2_balance_top=38.599, expert2_balance_bottom=14.119, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.72, wps=2835.3, ups=0.8, wpb=3527, bsz=148.8, num_updates=7800, lr=0.000358057, gnorm=0.901, loss_scale=16, train_wall=23, gb_free=9.1, wall=2643
2022-12-16 19:15:43 | INFO | train_inner | epoch 008:    189 / 1102 loss=3.997, nll_loss=2.504, moe_gate_loss=1.07904, cmr_gate_loss=0, overflow_expert1=0.001, overflow_expert2=19.821, entropy_gating=0.586, expert1_balance_top=32.323, expert1_balance_bottom=15.02, unused_expert1_count=0.002, expert2_balance_top=39.332, expert2_balance_bottom=14.01, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.67, wps=15479.9, ups=4.29, wpb=3605.1, bsz=149, num_updates=7900, lr=0.000355784, gnorm=0.905, loss_scale=16, train_wall=23, gb_free=8.8, wall=2666
2022-12-16 19:16:06 | INFO | train_inner | epoch 008:    289 / 1102 loss=4.008, nll_loss=2.517, moe_gate_loss=1.07872, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.554, entropy_gating=0.583, expert1_balance_top=32.293, expert1_balance_bottom=15.126, unused_expert1_count=0, expert2_balance_top=39.262, expert2_balance_bottom=14.08, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.73, wps=15284.8, ups=4.27, wpb=3583, bsz=151, num_updates=8000, lr=0.000353553, gnorm=0.925, loss_scale=16, train_wall=23, gb_free=8.7, wall=2690
2022-12-16 19:16:30 | INFO | train_inner | epoch 008:    389 / 1102 loss=4.037, nll_loss=2.547, moe_gate_loss=1.07816, cmr_gate_loss=0, overflow_expert1=0.002, overflow_expert2=19.796, entropy_gating=0.582, expert1_balance_top=32.098, expert1_balance_bottom=15.029, unused_expert1_count=0, expert2_balance_top=39.281, expert2_balance_bottom=13.994, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.85, wps=15386, ups=4.24, wpb=3626.8, bsz=144.5, num_updates=8100, lr=0.000351364, gnorm=0.888, loss_scale=16, train_wall=23, gb_free=8.8, wall=2713
2022-12-16 19:16:53 | INFO | train_inner | epoch 008:    489 / 1102 loss=4.03, nll_loss=2.541, moe_gate_loss=1.07814, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.791, entropy_gating=0.576, expert1_balance_top=32.161, expert1_balance_bottom=15.159, unused_expert1_count=0, expert2_balance_top=38.716, expert2_balance_bottom=14.292, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.82, wps=15670.5, ups=4.33, wpb=3617.9, bsz=152.7, num_updates=8200, lr=0.000349215, gnorm=0.882, loss_scale=16, train_wall=23, gb_free=8.7, wall=2736
2022-12-16 19:17:16 | INFO | train_inner | epoch 008:    589 / 1102 loss=4.043, nll_loss=2.556, moe_gate_loss=1.07771, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=19.617, entropy_gating=0.577, expert1_balance_top=32.288, expert1_balance_bottom=15.21, unused_expert1_count=0, expert2_balance_top=38.698, expert2_balance_bottom=14.175, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.88, wps=15438, ups=4.3, wpb=3592.5, bsz=143.8, num_updates=8300, lr=0.000347105, gnorm=0.923, loss_scale=16, train_wall=23, gb_free=8.8, wall=2760
2022-12-16 19:17:39 | INFO | train_inner | epoch 008:    689 / 1102 loss=4.024, nll_loss=2.535, moe_gate_loss=1.07861, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.189, entropy_gating=0.58, expert1_balance_top=32.127, expert1_balance_bottom=15.222, unused_expert1_count=0, expert2_balance_top=38.878, expert2_balance_bottom=13.827, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.8, wps=15286.9, ups=4.3, wpb=3551.3, bsz=150.9, num_updates=8400, lr=0.000345033, gnorm=0.925, loss_scale=16, train_wall=23, gb_free=9, wall=2783
2022-12-16 19:18:02 | INFO | train_inner | epoch 008:    789 / 1102 loss=4.064, nll_loss=2.579, moe_gate_loss=1.07734, cmr_gate_loss=0, overflow_expert1=0, overflow_expert2=20.07, entropy_gating=0.583, expert1_balance_top=32.074, expert1_balance_bottom=14.978, unused_expert1_count=0, expert2_balance_top=38.956, expert2_balance_bottom=13.901, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.97, wps=15248.9, ups=4.3, wpb=3543, bsz=139, num_updates=8500, lr=0.000342997, gnorm=0.913, loss_scale=16, train_wall=23, gb_free=8.9, wall=2806
2022-12-16 19:18:25 | INFO | train_inner | epoch 008:    889 / 1102 loss=4.063, nll_loss=2.578, moe_gate_loss=1.07971, cmr_gate_loss=0, overflow_expert1=0.001, overflow_expert2=19.96, entropy_gating=0.58, expert1_balance_top=32.294, expert1_balance_bottom=14.947, unused_expert1_count=0, expert2_balance_top=39.171, expert2_balance_bottom=13.821, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.97, wps=15647.7, ups=4.38, wpb=3572, bsz=136.4, num_updates=8600, lr=0.000340997, gnorm=0.926, loss_scale=16, train_wall=22, gb_free=8.9, wall=2829
2022-12-16 19:18:48 | INFO | train_inner | epoch 008:    989 / 1102 loss=4.043, nll_loss=2.557, moe_gate_loss=1.08162, cmr_gate_loss=0, overflow_expert1=0.014, overflow_expert2=19.879, entropy_gating=0.582, expert1_balance_top=32.638, expert1_balance_bottom=14.875, unused_expert1_count=0, expert2_balance_top=39.121, expert2_balance_bottom=14, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=5.88, wps=16257.6, ups=4.48, wpb=3627.2, bsz=151.9, num_updates=8700, lr=0.000339032, gnorm=0.925, loss_scale=16, train_wall=22, gb_free=8.6, wall=2851
2022-12-16 19:19:11 | INFO | train_inner | epoch 008:   1089 / 1102 loss=4.092, nll_loss=2.612, moe_gate_loss=1.07713, cmr_gate_loss=0, overflow_expert1=0.011, overflow_expert2=19.419, entropy_gating=0.592, expert1_balance_top=32.166, expert1_balance_bottom=15.155, unused_expert1_count=0, expert2_balance_top=38.528, expert2_balance_bottom=14.318, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, median_prefix_count_expert1=0, cmr_lang_gates=0, median_prefix_count_expert1_encoder=0, median_prefix_count_expert1_decoder=0, median_prefix_count_expert1_encoder_1st_layer=0, median_prefix_count_expert1_encoder_last_layer=0, median_prefix_count_expert1_decoder_1st_layer=0, median_prefix_count_expert1_decoder_last_layer=0, ppl=6.11, wps=15266.4, ups=4.27, wpb=3572.3, bsz=134.4, num_updates=8800, lr=0.0003371, gnorm=0.901, loss_scale=16, train_wall=23, gb_free=8.9, wall=2875
2022-12-16 19:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 19:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2022-12-16 19:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 19:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 19:20:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.297 | nll_loss 2.736 | moe_gate_loss 1.07212 | cmr_gate_loss 0 | overflow_expert1 11.227 | overflow_expert2 90.381 | entropy_gating 0.604 | expert1_balance_top 32.224 | expert1_balance_bottom 15.712 | unused_expert1_count 0 | expert2_balance_top 38.369 | expert2_balance_bottom 14.854 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | median_prefix_count_expert1 0 | cmr_lang_gates 0 | median_prefix_count_expert1_encoder 0 | median_prefix_count_expert1_decoder 0 | median_prefix_count_expert1_encoder_1st_layer 0 | median_prefix_count_expert1_encoder_last_layer 0 | median_prefix_count_expert1_decoder_1st_layer 0 | median_prefix_count_expert1_decoder_last_layer 0
 | ppl 6.66 | bleu 30.25 | wps 1907.5 | wpb 2835.3 | bsz 115.6 | num_updates 8813 | best_bleu 31.51
2022-12-16 19:20:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8813 updates
2022-12-16 19:20:48 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint8.pt

Process finished with exit code -1073740791 (0xC0000409)
