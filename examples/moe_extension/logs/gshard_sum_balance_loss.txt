D:\conda\envs\nmt\python.exe D:/nlper/nmt/fairseq/train.py data-bin/iwslt14 --max-tokens 4096 --user-dir moe_extension --task moe_translation --aux-w 0.01 --arch moe_transformer_iwslt_de_en --moe-freq 2 --moe-expert-count 4 --fp16 --optimizer adam --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.2 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy_w_moe --label-smoothing 0.1 --eval-bleu --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --num-workers 0
^D
2022-12-16 11:19:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'moe_extension', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='moe_transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='moe_transformer_iwslt_de_en', attention_dropout=0.0, aux_w=0.01, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_w_moe', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, der_coef=0.5, der_eps=0.1, der_tau=1.0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, entity_dict=None, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, moe_expert_count=4, moe_freq=2, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=0, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='moe_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, topk=-1, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_der=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='moe_extension', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'moe_translation', 'data': 'data-bin/iwslt14', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'use_der': False, 'entity_dict': None, 'topk': -1, 'der_coef': 0.5, 'der_tau': 1.0, 'der_eps': 0.1, 'aux_w': 0.01}, 'criterion': {'_name': 'label_smoothed_cross_entropy_w_moe', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-12-16 11:19:54 | INFO | moe_extension.tasks.moe_translation | [de] dictionary: 10152 types
2022-12-16 11:19:54 | INFO | moe_extension.tasks.moe_translation | [en] dictionary: 10152 types
2022-12-16 11:19:54 | INFO | fairseq_cli.train | TransformerModel(
2022-12-16 11:19:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-16 11:19:55 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 12.000 GB ; name = NVIDIA GeForce RTX 3060
2022-12-16 11:19:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-16 11:19:55 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-16 11:19:55 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2022-12-16 11:19:55 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints\checkpoint_last.pt
2022-12-16 11:19:55 | INFO | fairseq.trainer | No existing checkpoint found checkpoints\checkpoint_last.pt
2022-12-16 11:19:55 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-16 11:19:55 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14\train.de-en.de
2022-12-16 11:19:55 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14\train.de-en.en
2022-12-16 11:19:55 | INFO | moe_extension.tasks.moe_translation | data-bin/iwslt14 train de-en 160239 examples
2022-12-16 11:19:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:19:56 | INFO | fairseq.trainer | begin training epoch 1
2022-12-16 11:19:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:19:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-16 11:20:18 | INFO | train_inner | epoch 001:    101 / 1102 loss=12.64, nll_loss=12.459, aux_loss=1.234, ppl=5631.5, wps=16416.1, ups=4.68, wpb=3505.2, bsz=119.7, num_updates=100, lr=1.25e-05, gnorm=3.609, loss_scale=64, train_wall=22, gb_free=8.7, wall=24
2022-12-16 11:20:39 | INFO | train_inner | epoch 001:    201 / 1102 loss=10.972, nll_loss=10.597, aux_loss=1.156, ppl=1549.31, wps=16892.4, ups=4.74, wpb=3564.4, bsz=142.1, num_updates=200, lr=2.5e-05, gnorm=1.659, loss_scale=64, train_wall=21, gb_free=8.7, wall=45
2022-12-16 11:21:01 | INFO | train_inner | epoch 001:    301 / 1102 loss=10.111, nll_loss=9.606, aux_loss=1.086, ppl=779.34, wps=16800.9, ups=4.71, wpb=3563.5, bsz=133.4, num_updates=300, lr=3.75e-05, gnorm=1.642, loss_scale=64, train_wall=21, gb_free=9, wall=66
2022-12-16 11:21:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-16 11:21:22 | INFO | train_inner | epoch 001:    402 / 1102 loss=9.582, nll_loss=8.969, aux_loss=1.059, ppl=501.19, wps=17029.7, ups=4.63, wpb=3680, bsz=159, num_updates=400, lr=5e-05, gnorm=1.764, loss_scale=32, train_wall=21, gb_free=8.6, wall=88
2022-12-16 11:21:44 | INFO | train_inner | epoch 001:    502 / 1102 loss=9.379, nll_loss=8.714, aux_loss=1.063, ppl=420.04, wps=17033.9, ups=4.69, wpb=3635.6, bsz=152.6, num_updates=500, lr=6.25e-05, gnorm=1.867, loss_scale=32, train_wall=21, gb_free=9, wall=109
2022-12-16 11:22:05 | INFO | train_inner | epoch 001:    602 / 1102 loss=9.032, nll_loss=8.313, aux_loss=1.06, ppl=318.1, wps=16514.8, ups=4.59, wpb=3597.4, bsz=157.4, num_updates=600, lr=7.5e-05, gnorm=1.821, loss_scale=32, train_wall=21, gb_free=8.9, wall=131
2022-12-16 11:22:27 | INFO | train_inner | epoch 001:    702 / 1102 loss=8.773, nll_loss=8.009, aux_loss=1.064, ppl=257.6, wps=16470.5, ups=4.62, wpb=3567.7, bsz=160.9, num_updates=700, lr=8.75e-05, gnorm=1.854, loss_scale=32, train_wall=21, gb_free=8.7, wall=152
2022-12-16 11:22:48 | INFO | train_inner | epoch 001:    802 / 1102 loss=8.657, nll_loss=7.875, aux_loss=1.044, ppl=234.74, wps=16741.5, ups=4.67, wpb=3587.1, bsz=137, num_updates=800, lr=0.0001, gnorm=1.662, loss_scale=32, train_wall=21, gb_free=8.8, wall=174
2022-12-16 11:23:10 | INFO | train_inner | epoch 001:    902 / 1102 loss=8.488, nll_loss=7.677, aux_loss=1.046, ppl=204.66, wps=16412.6, ups=4.63, wpb=3547.7, bsz=140.4, num_updates=900, lr=0.0001125, gnorm=1.733, loss_scale=32, train_wall=21, gb_free=8.9, wall=195
2022-12-16 11:23:31 | INFO | train_inner | epoch 001:   1002 / 1102 loss=8.311, nll_loss=7.474, aux_loss=1.044, ppl=177.76, wps=16815.4, ups=4.67, wpb=3601.3, bsz=154.1, num_updates=1000, lr=0.000125, gnorm=1.73, loss_scale=32, train_wall=21, gb_free=8.9, wall=217
2022-12-16 11:23:53 | INFO | train_inner | epoch 001:   1102 / 1102 loss=8.231, nll_loss=7.38, aux_loss=1.054, ppl=166.59, wps=16688.6, ups=4.69, wpb=3561.5, bsz=141.5, num_updates=1100, lr=0.0001375, gnorm=1.742, loss_scale=32, train_wall=21, gb_free=8.9, wall=238
2022-12-16 11:23:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:28:43 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.071 | nll_loss 7.191 | aux_loss 1.042 | ppl 146.07 | bleu 4.02 | wps 611.8 | wpb 2835.3 | bsz 115.6 | num_updates 1100
2022-12-16 11:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1100 updates
2022-12-16 11:28:43 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint1.pt
2022-12-16 11:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint1.pt
2022-12-16 11:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint1.pt (epoch 1 @ 1100 updates, score 4.02) (writing took 2.9520235999999613 seconds)
2022-12-16 11:28:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-12-16 11:28:46 | INFO | train | epoch 001 | loss 9.464 | nll_loss 8.818 | aux_loss 1.038 | ppl 451.32 | wps 7439.5 | ups 2.08 | wpb 3582.9 | bsz 145.3 | num_updates 1100 | lr 0.0001375 | gnorm 1.917 | loss_scale 32 | train_wall 233 | gb_free 8.9 | wall 532
2022-12-16 11:28:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:28:47 | INFO | fairseq.trainer | begin training epoch 2
2022-12-16 11:28:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:29:09 | INFO | train_inner | epoch 002:    100 / 1102 loss=7.953, nll_loss=7.064, aux_loss=1.039, ppl=133.82, wps=1154.6, ups=0.32, wpb=3646.3, bsz=162, num_updates=1200, lr=0.00015, gnorm=1.532, loss_scale=32, train_wall=22, gb_free=8.9, wall=554
2022-12-16 11:29:30 | INFO | train_inner | epoch 002:    200 / 1102 loss=7.907, nll_loss=7.008, aux_loss=1.039, ppl=128.74, wps=16512.3, ups=4.61, wpb=3581, bsz=144.6, num_updates=1300, lr=0.0001625, gnorm=1.573, loss_scale=32, train_wall=21, gb_free=8.7, wall=576
2022-12-16 11:29:52 | INFO | train_inner | epoch 002:    300 / 1102 loss=7.723, nll_loss=6.797, aux_loss=1.044, ppl=111.17, wps=16543.9, ups=4.67, wpb=3540.3, bsz=161.2, num_updates=1400, lr=0.000175, gnorm=1.72, loss_scale=32, train_wall=21, gb_free=8.8, wall=597
2022-12-16 11:30:13 | INFO | train_inner | epoch 002:    400 / 1102 loss=7.715, nll_loss=6.785, aux_loss=1.048, ppl=110.26, wps=16700.5, ups=4.68, wpb=3570.1, bsz=129.4, num_updates=1500, lr=0.0001875, gnorm=1.511, loss_scale=32, train_wall=21, gb_free=8.9, wall=618
2022-12-16 11:30:34 | INFO | train_inner | epoch 002:    500 / 1102 loss=7.495, nll_loss=6.534, aux_loss=1.051, ppl=92.69, wps=16953.1, ups=4.7, wpb=3603.8, bsz=140.3, num_updates=1600, lr=0.0002, gnorm=1.554, loss_scale=32, train_wall=21, gb_free=8.8, wall=640
2022-12-16 11:30:56 | INFO | train_inner | epoch 002:    600 / 1102 loss=7.316, nll_loss=6.329, aux_loss=1.049, ppl=80.41, wps=16841.8, ups=4.69, wpb=3591.6, bsz=148.4, num_updates=1700, lr=0.0002125, gnorm=1.578, loss_scale=32, train_wall=21, gb_free=8.8, wall=661
2022-12-16 11:31:17 | INFO | train_inner | epoch 002:    700 / 1102 loss=7.154, nll_loss=6.142, aux_loss=1.055, ppl=70.62, wps=16452.9, ups=4.64, wpb=3547.1, bsz=149, num_updates=1800, lr=0.000225, gnorm=1.571, loss_scale=32, train_wall=21, gb_free=8.7, wall=683
2022-12-16 11:31:39 | INFO | train_inner | epoch 002:    800 / 1102 loss=6.999, nll_loss=5.964, aux_loss=1.05, ppl=62.4, wps=16580.8, ups=4.6, wpb=3607.1, bsz=146.7, num_updates=1900, lr=0.0002375, gnorm=1.495, loss_scale=32, train_wall=21, gb_free=8.7, wall=704
2022-12-16 11:32:00 | INFO | train_inner | epoch 002:    900 / 1102 loss=6.987, nll_loss=5.947, aux_loss=1.059, ppl=61.7, wps=16860.2, ups=4.68, wpb=3600.7, bsz=137.8, num_updates=2000, lr=0.00025, gnorm=1.561, loss_scale=32, train_wall=21, gb_free=8.9, wall=726
2022-12-16 11:32:22 | INFO | train_inner | epoch 002:   1000 / 1102 loss=6.834, nll_loss=5.77, aux_loss=1.065, ppl=54.56, wps=16455.9, ups=4.68, wpb=3515.5, bsz=141.3, num_updates=2100, lr=0.0002625, gnorm=1.468, loss_scale=32, train_wall=21, gb_free=8.9, wall=747
2022-12-16 11:32:43 | INFO | train_inner | epoch 002:   1100 / 1102 loss=6.745, nll_loss=5.666, aux_loss=1.065, ppl=50.79, wps=17156.4, ups=4.73, wpb=3624.9, bsz=140.6, num_updates=2200, lr=0.000275, gnorm=1.49, loss_scale=32, train_wall=21, gb_free=8.8, wall=768
2022-12-16 11:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset

2022-12-16 11:35:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.437 | nll_loss 5.279 | aux_loss 1.07 | ppl 38.82 | bleu 10.5 | wps 1302.3 | wpb 2835.3 | bsz 115.6 | num_updates 2202 | best_bleu 10.5
2022-12-16 11:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2202 updates
2022-12-16 11:35:00 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint2.pt
2022-12-16 11:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint2.pt
2022-12-16 11:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint2.pt (epoch 2 @ 2202 updates, score 10.5) (writing took 3.184619399999974 seconds)
2022-12-16 11:35:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-12-16 11:35:03 | INFO | train | epoch 002 | loss 7.348 | nll_loss 6.365 | aux_loss 1.006 | ppl 82.41 | wps 10486.8 | ups 2.93 | wpb 3583.6 | bsz 145.4 | num_updates 2202 | lr 0.00027525 | gnorm 1.55 | loss_scale 32 | train_wall 233 | gb_free 8.8 | wall 908
2022-12-16 11:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:35:03 | INFO | fairseq.trainer | begin training epoch 3
2022-12-16 11:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:35:25 | INFO | train_inner | epoch 003:     98 / 1102 loss=6.509, nll_loss=5.4, aux_loss=1.065, ppl=42.22, wps=2227.9, ups=0.62, wpb=3604.3, bsz=140.9, num_updates=2300, lr=0.0002875, gnorm=1.392, loss_scale=32, train_wall=21, gb_free=8.8, wall=930
2022-12-16 11:35:46 | INFO | train_inner | epoch 003:    198 / 1102 loss=6.429, nll_loss=5.307, aux_loss=1.07, ppl=39.59, wps=16528.5, ups=4.65, wpb=3551.7, bsz=139.6, num_updates=2400, lr=0.0003, gnorm=1.429, loss_scale=32, train_wall=21, gb_free=8.8, wall=951
2022-12-16 11:36:08 | INFO | train_inner | epoch 003:    298 / 1102 loss=6.327, nll_loss=5.187, aux_loss=1.071, ppl=36.44, wps=16441.5, ups=4.57, wpb=3595.1, bsz=146.2, num_updates=2500, lr=0.0003125, gnorm=1.431, loss_scale=32, train_wall=21, gb_free=8.6, wall=973
2022-12-16 11:36:29 | INFO | train_inner | epoch 003:    398 / 1102 loss=6.23, nll_loss=5.073, aux_loss=1.066, ppl=33.65, wps=17049, ups=4.73, wpb=3607.5, bsz=141.5, num_updates=2600, lr=0.000325, gnorm=1.38, loss_scale=32, train_wall=21, gb_free=8.8, wall=994
2022-12-16 11:36:50 | INFO | train_inner | epoch 003:    498 / 1102 loss=6.177, nll_loss=5.011, aux_loss=1.07, ppl=32.25, wps=16793, ups=4.67, wpb=3594.3, bsz=134.9, num_updates=2700, lr=0.0003375, gnorm=1.334, loss_scale=32, train_wall=21, gb_free=8.7, wall=1016
2022-12-16 11:37:12 | INFO | train_inner | epoch 003:    598 / 1102 loss=6.012, nll_loss=4.821, aux_loss=1.07, ppl=28.28, wps=16209, ups=4.6, wpb=3521.5, bsz=150.2, num_updates=2800, lr=0.00035, gnorm=1.398, loss_scale=32, train_wall=21, gb_free=8.9, wall=1038
2022-12-16 11:37:35 | INFO | train_inner | epoch 003:    698 / 1102 loss=5.882, nll_loss=4.67, aux_loss=1.073, ppl=25.46, wps=16159, ups=4.47, wpb=3611.9, bsz=159.4, num_updates=2900, lr=0.0003625, gnorm=1.377, loss_scale=32, train_wall=22, gb_free=8.6, wall=1060
2022-12-16 11:37:57 | INFO | train_inner | epoch 003:    798 / 1102 loss=5.835, nll_loss=4.616, aux_loss=1.072, ppl=24.51, wps=16156.3, ups=4.49, wpb=3601, bsz=154.4, num_updates=3000, lr=0.000375, gnorm=1.34, loss_scale=32, train_wall=22, gb_free=8.7, wall=1082
2022-12-16 11:38:18 | INFO | train_inner | epoch 003:    898 / 1102 loss=5.91, nll_loss=4.698, aux_loss=1.07, ppl=25.95, wps=16631.2, ups=4.7, wpb=3541.2, bsz=124, num_updates=3100, lr=0.0003875, gnorm=1.338, loss_scale=32, train_wall=21, gb_free=8.6, wall=1104
2022-12-16 11:38:40 | INFO | train_inner | epoch 003:    998 / 1102 loss=5.727, nll_loss=4.488, aux_loss=1.075, ppl=22.44, wps=16592.3, ups=4.57, wpb=3633.6, bsz=152.4, num_updates=3200, lr=0.0004, gnorm=1.354, loss_scale=32, train_wall=22, gb_free=8.6, wall=1125
2022-12-16 11:39:02 | INFO | train_inner | epoch 003:   1098 / 1102 loss=5.658, nll_loss=4.408, aux_loss=1.084, ppl=21.23, wps=16397.8, ups=4.62, wpb=3546.6, bsz=153.4, num_updates=3300, lr=0.0004125, gnorm=1.354, loss_scale=32, train_wall=21, gb_free=9.1, wall=1147
2022-12-16 11:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:40:52 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.566 | nll_loss 4.24 | aux_loss 1.086 | ppl 18.9 | bleu 15.04 | wps 1626.6 | wpb 2835.3 | bsz 115.6 | num_updates 3304 | best_bleu 15.04
2022-12-16 11:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3304 updates
2022-12-16 11:40:52 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint3.pt
2022-12-16 11:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint3.pt
2022-12-16 11:40:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint3.pt (epoch 3 @ 3304 updates, score 15.04) (writing took 3.0383159000000433 seconds)
2022-12-16 11:40:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-12-16 11:40:55 | INFO | train | epoch 003 | loss 6.059 | nll_loss 4.875 | aux_loss 1.02 | ppl 29.35 | wps 11216.2 | ups 3.13 | wpb 3583.6 | bsz 145.4 | num_updates 3304 | lr 0.000413 | gnorm 1.375 | loss_scale 32 | train_wall 235 | gb_free 8.6 | wall 1261
2022-12-16 11:40:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:40:55 | INFO | fairseq.trainer | begin training epoch 4
2022-12-16 11:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:41:16 | INFO | train_inner | epoch 004:     96 / 1102 loss=5.438, nll_loss=4.157, aux_loss=1.079, ppl=17.84, wps=2661.6, ups=0.74, wpb=3583.7, bsz=150.9, num_updates=3400, lr=0.000425, gnorm=1.272, loss_scale=32, train_wall=22, gb_free=9, wall=1282
2022-12-16 11:41:38 | INFO | train_inner | epoch 004:    196 / 1102 loss=5.433, nll_loss=4.148, aux_loss=1.075, ppl=17.72, wps=16648.6, ups=4.7, wpb=3545.2, bsz=137.6, num_updates=3500, lr=0.0004375, gnorm=1.252, loss_scale=32, train_wall=21, gb_free=8.8, wall=1303
2022-12-16 11:41:59 | INFO | train_inner | epoch 004:    296 / 1102 loss=5.396, nll_loss=4.104, aux_loss=1.072, ppl=17.2, wps=16770.8, ups=4.6, wpb=3647.8, bsz=131.3, num_updates=3600, lr=0.00045, gnorm=1.265, loss_scale=32, train_wall=21, gb_free=8.8, wall=1325
2022-12-16 11:42:21 | INFO | train_inner | epoch 004:    396 / 1102 loss=5.405, nll_loss=4.112, aux_loss=1.077, ppl=17.29, wps=16302.7, ups=4.66, wpb=3496.7, bsz=136.1, num_updates=3700, lr=0.0004625, gnorm=1.279, loss_scale=32, train_wall=21, gb_free=8.9, wall=1346
2022-12-16 11:42:42 | INFO | train_inner | epoch 004:    496 / 1102 loss=5.326, nll_loss=4.02, aux_loss=1.076, ppl=16.22, wps=17104.4, ups=4.73, wpb=3618.5, bsz=138.7, num_updates=3800, lr=0.000475, gnorm=1.231, loss_scale=32, train_wall=21, gb_free=8.8, wall=1367
2022-12-16 11:43:03 | INFO | train_inner | epoch 004:    596 / 1102 loss=5.288, nll_loss=3.977, aux_loss=1.072, ppl=15.75, wps=16738, ups=4.67, wpb=3581.8, bsz=142.1, num_updates=3900, lr=0.0004875, gnorm=1.24, loss_scale=32, train_wall=21, gb_free=8.9, wall=1389
2022-12-16 11:43:25 | INFO | train_inner | epoch 004:    696 / 1102 loss=5.249, nll_loss=3.932, aux_loss=1.073, ppl=15.26, wps=16779.1, ups=4.64, wpb=3612.9, bsz=145, num_updates=4000, lr=0.0005, gnorm=1.214, loss_scale=32, train_wall=21, gb_free=8.9, wall=1410
2022-12-16 11:43:46 | INFO | train_inner | epoch 004:    796 / 1102 loss=5.197, nll_loss=3.873, aux_loss=1.074, ppl=14.65, wps=16744.8, ups=4.74, wpb=3531.8, bsz=161.1, num_updates=4100, lr=0.000493865, gnorm=1.338, loss_scale=32, train_wall=21, gb_free=8.7, wall=1431
2022-12-16 11:44:07 | INFO | train_inner | epoch 004:    896 / 1102 loss=5.184, nll_loss=3.855, aux_loss=1.077, ppl=14.47, wps=16700.8, ups=4.67, wpb=3574.2, bsz=152.9, num_updates=4200, lr=0.00048795, gnorm=1.243, loss_scale=32, train_wall=21, gb_free=8.8, wall=1453
2022-12-16 11:44:29 | INFO | train_inner | epoch 004:    996 / 1102 loss=5.103, nll_loss=3.762, aux_loss=1.076, ppl=13.57, wps=17140.3, ups=4.7, wpb=3649.1, bsz=144.9, num_updates=4300, lr=0.000482243, gnorm=1.142, loss_scale=32, train_wall=21, gb_free=9, wall=1474
2022-12-16 11:44:50 | INFO | train_inner | epoch 004:   1096 / 1102 loss=4.974, nll_loss=3.619, aux_loss=1.074, ppl=12.28, wps=16520.6, ups=4.61, wpb=3586.2, bsz=160.7, num_updates=4400, lr=0.000476731, gnorm=1.121, loss_scale=32, train_wall=21, gb_free=9, wall=1496
2022-12-16 11:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:46:26 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.921 | nll_loss 3.464 | aux_loss 1.079 | ppl 11.03 | bleu 23.86 | wps 1888.1 | wpb 2835.3 | bsz 115.6 | num_updates 4406 | best_bleu 23.86
2022-12-16 11:46:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4406 updates
2022-12-16 11:46:26 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint4.pt
2022-12-16 11:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint4.pt
2022-12-16 11:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint4.pt (epoch 4 @ 4406 updates, score 23.86) (writing took 3.0987886999998864 seconds)
2022-12-16 11:46:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-12-16 11:46:29 | INFO | train | epoch 004 | loss 5.271 | nll_loss 3.958 | aux_loss 1.021 | ppl 15.54 | wps 11821.4 | ups 3.3 | wpb 3583.6 | bsz 145.4 | num_updates 4406 | lr 0.000476407 | gnorm 1.235 | loss_scale 32 | train_wall 233 | gb_free 8.7 | wall 1595
2022-12-16 11:46:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:46:29 | INFO | fairseq.trainer | begin training epoch 5
2022-12-16 11:46:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:46:50 | INFO | train_inner | epoch 005:     94 / 1102 loss=4.884, nll_loss=3.515, aux_loss=1.075, ppl=11.43, wps=2998.1, ups=0.83, wpb=3593.8, bsz=151.7, num_updates=4500, lr=0.000471405, gnorm=1.159, loss_scale=32, train_wall=22, gb_free=8.9, wall=1616
2022-12-16 11:47:12 | INFO | train_inner | epoch 005:    194 / 1102 loss=4.784, nll_loss=3.4, aux_loss=1.072, ppl=10.56, wps=16828.9, ups=4.66, wpb=3607.7, bsz=152.5, num_updates=4600, lr=0.000466252, gnorm=1.071, loss_scale=32, train_wall=21, gb_free=8.7, wall=1637
2022-12-16 11:47:33 | INFO | train_inner | epoch 005:    294 / 1102 loss=4.805, nll_loss=3.423, aux_loss=1.073, ppl=10.73, wps=16749.7, ups=4.64, wpb=3609.2, bsz=144.2, num_updates=4700, lr=0.000461266, gnorm=1.079, loss_scale=32, train_wall=21, gb_free=9, wall=1659
2022-12-16 11:47:55 | INFO | train_inner | epoch 005:    394 / 1102 loss=4.763, nll_loss=3.376, aux_loss=1.071, ppl=10.39, wps=16374.3, ups=4.63, wpb=3535.2, bsz=143.8, num_updates=4800, lr=0.000456435, gnorm=1.052, loss_scale=32, train_wall=21, gb_free=9, wall=1680
2022-12-16 11:48:17 | INFO | train_inner | epoch 005:    494 / 1102 loss=4.851, nll_loss=3.474, aux_loss=1.066, ppl=11.12, wps=16255.9, ups=4.61, wpb=3523.9, bsz=126.9, num_updates=4900, lr=0.000451754, gnorm=1.126, loss_scale=32, train_wall=21, gb_free=8.7, wall=1702
2022-12-16 11:48:38 | INFO | train_inner | epoch 005:    594 / 1102 loss=4.818, nll_loss=3.438, aux_loss=1.072, ppl=10.83, wps=16349.1, ups=4.65, wpb=3517.5, bsz=133.8, num_updates=5000, lr=0.000447214, gnorm=1.098, loss_scale=32, train_wall=21, gb_free=8.9, wall=1723
2022-12-16 11:49:00 | INFO | train_inner | epoch 005:    694 / 1102 loss=4.6, nll_loss=3.19, aux_loss=1.071, ppl=9.13, wps=16777.3, ups=4.55, wpb=3689.6, bsz=164.4, num_updates=5100, lr=0.000442807, gnorm=0.989, loss_scale=32, train_wall=22, gb_free=8.7, wall=1745
2022-12-16 11:49:22 | INFO | train_inner | epoch 005:    794 / 1102 loss=4.71, nll_loss=3.316, aux_loss=1.07, ppl=9.96, wps=16211.4, ups=4.53, wpb=3580.1, bsz=140.3, num_updates=5200, lr=0.000438529, gnorm=1.027, loss_scale=32, train_wall=22, gb_free=8.7, wall=1768
2022-12-16 11:49:44 | INFO | train_inner | epoch 005:    894 / 1102 loss=4.685, nll_loss=3.287, aux_loss=1.075, ppl=9.76, wps=16010.7, ups=4.52, wpb=3544.2, bsz=154.1, num_updates=5300, lr=0.000434372, gnorm=1.137, loss_scale=32, train_wall=22, gb_free=8.7, wall=1790
2022-12-16 11:50:06 | INFO | train_inner | epoch 005:    994 / 1102 loss=4.683, nll_loss=3.284, aux_loss=1.068, ppl=9.74, wps=16460.8, ups=4.6, wpb=3580.8, bsz=137.9, num_updates=5400, lr=0.000430331, gnorm=1.026, loss_scale=32, train_wall=21, gb_free=8.7, wall=1811
2022-12-16 11:50:28 | INFO | train_inner | epoch 005:   1094 / 1102 loss=4.618, nll_loss=3.212, aux_loss=1.073, ppl=9.26, wps=16454, ups=4.53, wpb=3629.2, bsz=151, num_updates=5500, lr=0.000426401, gnorm=1.032, loss_scale=32, train_wall=22, gb_free=8.9, wall=1833
2022-12-16 11:50:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 11:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 11:52:00 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.631 | nll_loss 3.135 | aux_loss 1.081 | ppl 8.79 | bleu 26.38 | wps 1982.4 | wpb 2835.3 | bsz 115.6 | num_updates 5508 | best_bleu 26.38
2022-12-16 11:52:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5508 updates
2022-12-16 11:52:00 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint5.pt
2022-12-16 11:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint5.pt
2022-12-16 11:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint5.pt (epoch 5 @ 5508 updates, score 26.38) (writing took 3.463294700000006 seconds)
2022-12-16 11:52:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-12-16 11:52:03 | INFO | train | epoch 005 | loss 4.742 | nll_loss 3.352 | aux_loss 1.02 | ppl 10.21 | wps 11823.4 | ups 3.3 | wpb 3583.6 | bsz 145.4 | num_updates 5508 | lr 0.000426092 | gnorm 1.072 | loss_scale 32 | train_wall 236 | gb_free 9.1 | wall 1929
2022-12-16 11:52:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:52:03 | INFO | fairseq.trainer | begin training epoch 6
2022-12-16 11:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:52:23 | INFO | train_inner | epoch 006:     92 / 1102 loss=4.486, nll_loss=3.063, aux_loss=1.076, ppl=8.35, wps=3055.4, ups=0.87, wpb=3515.4, bsz=139.8, num_updates=5600, lr=0.000422577, gnorm=1.034, loss_scale=32, train_wall=21, gb_free=8.8, wall=1949
2022-12-16 11:52:45 | INFO | train_inner | epoch 006:    192 / 1102 loss=4.444, nll_loss=3.013, aux_loss=1.072, ppl=8.07, wps=16405.8, ups=4.62, wpb=3549, bsz=154.6, num_updates=5700, lr=0.000418854, gnorm=1.021, loss_scale=32, train_wall=21, gb_free=8.7, wall=1970
2022-12-16 11:53:06 | INFO | train_inner | epoch 006:    292 / 1102 loss=4.444, nll_loss=3.012, aux_loss=1.07, ppl=8.07, wps=16316.4, ups=4.63, wpb=3523.5, bsz=137.3, num_updates=5800, lr=0.000415227, gnorm=0.997, loss_scale=32, train_wall=21, gb_free=8.9, wall=1992
2022-12-16 11:53:28 | INFO | train_inner | epoch 006:    392 / 1102 loss=4.375, nll_loss=2.932, aux_loss=1.067, ppl=7.63, wps=16710.4, ups=4.6, wpb=3636, bsz=151.8, num_updates=5900, lr=0.000411693, gnorm=0.973, loss_scale=32, train_wall=21, gb_free=8.8, wall=2013
2022-12-16 11:53:50 | INFO | train_inner | epoch 006:    492 / 1102 loss=4.414, nll_loss=2.978, aux_loss=1.065, ppl=7.88, wps=16724.9, ups=4.58, wpb=3651.4, bsz=141.3, num_updates=6000, lr=0.000408248, gnorm=0.955, loss_scale=32, train_wall=21, gb_free=8.8, wall=2035
2022-12-16 11:54:11 | INFO | train_inner | epoch 006:    592 / 1102 loss=4.471, nll_loss=3.042, aux_loss=1.066, ppl=8.24, wps=16521.8, ups=4.66, wpb=3544.7, bsz=133.8, num_updates=6100, lr=0.000404888, gnorm=0.993, loss_scale=32, train_wall=21, gb_free=8.8, wall=2057
2022-12-16 11:54:33 | INFO | train_inner | epoch 006:    692 / 1102 loss=4.371, nll_loss=2.93, aux_loss=1.07, ppl=7.62, wps=16600.6, ups=4.65, wpb=3572.4, bsz=158.6, num_updates=6200, lr=0.00040161, gnorm=0.962, loss_scale=32, train_wall=21, gb_free=8.8, wall=2078
2022-12-16 11:54:55 | INFO | train_inner | epoch 006:    792 / 1102 loss=4.289, nll_loss=2.838, aux_loss=1.065, ppl=7.15, wps=16671.7, ups=4.54, wpb=3675.1, bsz=163.4, num_updates=6300, lr=0.00039841, gnorm=0.906, loss_scale=32, train_wall=22, gb_free=8.9, wall=2100
2022-12-16 11:55:16 | INFO | train_inner | epoch 006:    892 / 1102 loss=4.445, nll_loss=3.014, aux_loss=1.066, ppl=8.08, wps=16789.3, ups=4.72, wpb=3558.9, bsz=134.5, num_updates=6400, lr=0.000395285, gnorm=0.982, loss_scale=32, train_wall=21, gb_free=9.1, wall=2122
2022-12-16 11:55:38 | INFO | train_inner | epoch 006:    992 / 1102 loss=4.429, nll_loss=2.996, aux_loss=1.066, ppl=7.98, wps=16785.9, ups=4.68, wpb=3587.9, bsz=135.1, num_updates=6500, lr=0.000392232, gnorm=0.968, loss_scale=32, train_wall=21, gb_free=8.8, wall=2143
2022-12-16 11:55:59 | INFO | train_inner | epoch 006:   1092 / 1102 loss=4.362, nll_loss=2.92, aux_loss=1.069, ppl=7.57, wps=16824.5, ups=4.67, wpb=3606.5, bsz=147, num_updates=6600, lr=0.000389249, gnorm=0.972, loss_scale=32, train_wall=21, gb_free=8.7, wall=2164
2022-12-16 11:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:57:23 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.474 | nll_loss 2.951 | aux_loss 1.071 | ppl 7.73 | bleu 24.97 | wps 2194.6 | wpb 2835.3 | bsz 115.6 | num_updates 6610 | best_bleu 26.38
2022-12-16 11:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6610 updates
2022-12-16 11:57:23 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint6.pt
2022-12-16 11:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint6.pt
2022-12-16 11:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint6.pt (epoch 6 @ 6610 updates, score 24.97) (writing took 2.174978900000042 seconds)
2022-12-16 11:57:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-12-16 11:57:25 | INFO | train | epoch 006 | loss 4.408 | nll_loss 2.972 | aux_loss 1.02 | ppl 7.85 | wps 12282.5 | ups 3.43 | wpb 3583.6 | bsz 145.4 | num_updates 6610 | lr 0.000388955 | gnorm 0.977 | loss_scale 32 | train_wall 234 | gb_free 8.7 | wall 2250
2022-12-16 11:57:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 11:57:25 | INFO | fairseq.trainer | begin training epoch 7
2022-12-16 11:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:57:44 | INFO | train_inner | epoch 007:     90 / 1102 loss=4.158, nll_loss=2.689, aux_loss=1.067, ppl=6.45, wps=3380.3, ups=0.95, wpb=3565.1, bsz=148.3, num_updates=6700, lr=0.000386334, gnorm=0.897, loss_scale=32, train_wall=21, gb_free=9.2, wall=2270
2022-12-16 11:58:06 | INFO | train_inner | epoch 007:    190 / 1102 loss=4.135, nll_loss=2.662, aux_loss=1.067, ppl=6.33, wps=16805.9, ups=4.64, wpb=3621.3, bsz=159.9, num_updates=6800, lr=0.000383482, gnorm=0.91, loss_scale=32, train_wall=21, gb_free=9.1, wall=2291
2022-12-16 11:58:27 | INFO | train_inner | epoch 007:    290 / 1102 loss=4.16, nll_loss=2.688, aux_loss=1.068, ppl=6.44, wps=16881.2, ups=4.66, wpb=3624.9, bsz=148.9, num_updates=6900, lr=0.000380693, gnorm=0.913, loss_scale=32, train_wall=21, gb_free=8.7, wall=2313
2022-12-16 11:58:49 | INFO | train_inner | epoch 007:    390 / 1102 loss=4.202, nll_loss=2.737, aux_loss=1.066, ppl=6.67, wps=16525.8, ups=4.55, wpb=3635.5, bsz=136.3, num_updates=7000, lr=0.000377964, gnorm=0.922, loss_scale=32, train_wall=22, gb_free=9.1, wall=2335
2022-12-16 11:59:11 | INFO | train_inner | epoch 007:    490 / 1102 loss=4.229, nll_loss=2.768, aux_loss=1.067, ppl=6.81, wps=16127.8, ups=4.63, wpb=3485.9, bsz=141.2, num_updates=7100, lr=0.000375293, gnorm=0.982, loss_scale=32, train_wall=21, gb_free=9.1, wall=2356
2022-12-16 11:59:33 | INFO | train_inner | epoch 007:    590 / 1102 loss=4.228, nll_loss=2.766, aux_loss=1.067, ppl=6.8, wps=16609.3, ups=4.65, wpb=3573.3, bsz=138.3, num_updates=7200, lr=0.000372678, gnorm=0.942, loss_scale=32, train_wall=21, gb_free=8.8, wall=2378
2022-12-16 11:59:54 | INFO | train_inner | epoch 007:    690 / 1102 loss=4.256, nll_loss=2.798, aux_loss=1.066, ppl=6.96, wps=16598.7, ups=4.72, wpb=3518.1, bsz=139.8, num_updates=7300, lr=0.000370117, gnorm=0.98, loss_scale=32, train_wall=21, gb_free=8.8, wall=2399
2022-12-16 12:00:16 | INFO | train_inner | epoch 007:    790 / 1102 loss=4.164, nll_loss=2.694, aux_loss=1.065, ppl=6.47, wps=16646.5, ups=4.57, wpb=3643.2, bsz=149.1, num_updates=7400, lr=0.000367607, gnorm=0.914, loss_scale=32, train_wall=22, gb_free=8.8, wall=2421
2022-12-16 12:00:37 | INFO | train_inner | epoch 007:    890 / 1102 loss=4.184, nll_loss=2.718, aux_loss=1.067, ppl=6.58, wps=16257.6, ups=4.6, wpb=3530.6, bsz=154.9, num_updates=7500, lr=0.000365148, gnorm=0.927, loss_scale=32, train_wall=21, gb_free=8.8, wall=2443
2022-12-16 12:01:00 | INFO | train_inner | epoch 007:    990 / 1102 loss=4.186, nll_loss=2.72, aux_loss=1.066, ppl=6.59, wps=15956.7, ups=4.4, wpb=3630.6, bsz=142.3, num_updates=7600, lr=0.000362738, gnorm=0.911, loss_scale=32, train_wall=22, gb_free=8.8, wall=2466
2022-12-16 12:01:22 | INFO | train_inner | epoch 007:   1090 / 1102 loss=4.195, nll_loss=2.731, aux_loss=1.065, ppl=6.64, wps=16157.1, ups=4.5, wpb=3591, bsz=138.2, num_updates=7700, lr=0.000360375, gnorm=0.931, loss_scale=32, train_wall=22, gb_free=8.9, wall=2488
2022-12-16 12:01:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 12:03:06 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.404 | nll_loss 2.871 | aux_loss 1.075 | ppl 7.32 | bleu 32.41 | wps 1760.7 | wpb 2835.3 | bsz 115.6 | num_updates 7712 | best_bleu 32.41
2022-12-16 12:03:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7712 updates
2022-12-16 12:03:06 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint7.pt
2022-12-16 12:03:08 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint7.pt
2022-12-16 12:03:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint7.pt (epoch 7 @ 7712 updates, score 32.41) (writing took 3.2503092000001743 seconds)
2022-12-16 12:03:09 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-12-16 12:03:09 | INFO | train | epoch 007 | loss 4.188 | nll_loss 2.722 | aux_loss 1.019 | ppl 6.6 | wps 11454.1 | ups 3.2 | wpb 3583.6 | bsz 145.4 | num_updates 7712 | lr 0.000360095 | gnorm 0.93 | loss_scale 32 | train_wall 236 | gb_free 8.7 | wall 2595
2022-12-16 12:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 12:03:10 | INFO | fairseq.trainer | begin training epoch 8
2022-12-16 12:03:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 12:03:29 | INFO | train_inner | epoch 008:     88 / 1102 loss=4.009, nll_loss=2.52, aux_loss=1.067, ppl=5.73, wps=2791.5, ups=0.79, wpb=3535.7, bsz=148.4, num_updates=7800, lr=0.000358057, gnorm=0.903, loss_scale=32, train_wall=22, gb_free=8.8, wall=2614
2022-12-16 12:03:51 | INFO | train_inner | epoch 008:    188 / 1102 loss=3.992, nll_loss=2.497, aux_loss=1.063, ppl=5.65, wps=16151.5, ups=4.49, wpb=3597.9, bsz=149.5, num_updates=7900, lr=0.000355784, gnorm=0.9, loss_scale=32, train_wall=22, gb_free=8.7, wall=2637
2022-12-16 12:04:13 | INFO | train_inner | epoch 008:    288 / 1102 loss=3.998, nll_loss=2.506, aux_loss=1.066, ppl=5.68, wps=16398.8, ups=4.58, wpb=3580.3, bsz=151.1, num_updates=8000, lr=0.000353553, gnorm=0.922, loss_scale=32, train_wall=21, gb_free=8.8, wall=2659
2022-12-16 12:04:35 | INFO | train_inner | epoch 008:    388 / 1102 loss=4.038, nll_loss=2.549, aux_loss=1.065, ppl=5.85, wps=16764.3, ups=4.62, wpb=3625.9, bsz=143.4, num_updates=8100, lr=0.000351364, gnorm=0.895, loss_scale=32, train_wall=21, gb_free=8.8, wall=2680
2022-12-16 12:04:56 | INFO | train_inner | epoch 008:    488 / 1102 loss=4.026, nll_loss=2.538, aux_loss=1.065, ppl=5.81, wps=16821.5, ups=4.65, wpb=3617.6, bsz=152.7, num_updates=8200, lr=0.000349215, gnorm=0.884, loss_scale=32, train_wall=21, gb_free=9, wall=2702
2022-12-16 12:05:18 | INFO | train_inner | epoch 008:    588 / 1102 loss=4.034, nll_loss=2.546, aux_loss=1.065, ppl=5.84, wps=16688.5, ups=4.64, wpb=3596.1, bsz=144.4, num_updates=8300, lr=0.000347105, gnorm=0.911, loss_scale=32, train_wall=21, gb_free=9.1, wall=2723
2022-12-16 12:05:39 | INFO | train_inner | epoch 008:    688 / 1102 loss=4.018, nll_loss=2.527, aux_loss=1.063, ppl=5.76, wps=16439.3, ups=4.62, wpb=3555.3, bsz=151.6, num_updates=8400, lr=0.000345033, gnorm=0.931, loss_scale=32, train_wall=21, gb_free=8.8, wall=2745
2022-12-16 12:06:01 | INFO | train_inner | epoch 008:    788 / 1102 loss=4.064, nll_loss=2.58, aux_loss=1.063, ppl=5.98, wps=16513.1, ups=4.67, wpb=3536.6, bsz=137.7, num_updates=8500, lr=0.000342997, gnorm=0.916, loss_scale=32, train_wall=21, gb_free=8.8, wall=2766
2022-12-16 12:06:22 | INFO | train_inner | epoch 008:    888 / 1102 loss=4.059, nll_loss=2.574, aux_loss=1.063, ppl=5.95, wps=16665.5, ups=4.67, wpb=3571.7, bsz=136.4, num_updates=8600, lr=0.000340997, gnorm=0.925, loss_scale=32, train_wall=21, gb_free=8.7, wall=2788
2022-12-16 12:06:44 | INFO | train_inner | epoch 008:    988 / 1102 loss=4.03, nll_loss=2.543, aux_loss=1.068, ppl=5.83, wps=16982.1, ups=4.68, wpb=3625.7, bsz=152.6, num_updates=8700, lr=0.000339032, gnorm=0.898, loss_scale=32, train_wall=21, gb_free=8.7, wall=2809
2022-12-16 12:07:05 | INFO | train_inner | epoch 008:   1088 / 1102 loss=4.084, nll_loss=2.604, aux_loss=1.065, ppl=6.08, wps=16714.3, ups=4.67, wpb=3582.7, bsz=135.3, num_updates=8800, lr=0.0003371, gnorm=0.913, loss_scale=32, train_wall=21, gb_free=9, wall=2830
2022-12-16 12:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 12:08:37 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.278 | nll_loss 2.72 | aux_loss 1.062 | ppl 6.59 | bleu 31.29 | wps 2009.1 | wpb 2835.3 | bsz 115.6 | num_updates 8814 | best_bleu 32.41
2022-12-16 12:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8814 updates
2022-12-16 12:08:37 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint8.pt

Process finished with exit code -1073740791 (0xC0000409)
