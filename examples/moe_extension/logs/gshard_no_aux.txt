D:\conda\envs\nmt\python.exe D:/nlper/nmt/fairseq/train.py data-bin/iwslt14 --max-tokens 4096 --user-dir moe_extension --task moe_translation --aux-w 0. --arch moe_transformer_iwslt_de_en --moe-freq 2 --moe-expert-count 4 --fp16 --optimizer adam --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.2 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy_w_moe --label-smoothing 0.1 --eval-bleu --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --num-workers 0
2022-12-15 23:55:27 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'moe_extension', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='moe_transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='moe_transformer_iwslt_de_en', attention_dropout=0.0, aux_w=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_w_moe', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, der_coef=0.5, der_eps=0.1, der_tau=1.0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, entity_dict=None, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, moe_expert_count=4, moe_freq=2, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=0, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='moe_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, topk=-1, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_der=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='moe_extension', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'moe_translation', 'data': 'data-bin/iwslt14', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False, 'use_der': False, 'entity_dict': None, 'topk': -1, 'der_coef': 0.5, 'der_tau': 1.0, 'der_eps': 0.1, 'aux_w': 0.0}, 'criterion': {'_name': 'label_smoothed_cross_entropy_w_moe', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-12-15 23:55:27 | INFO | moe_extension.tasks.moe_translation | [de] dictionary: 10152 types
2022-12-15 23:55:27 | INFO | moe_extension.tasks.moe_translation | [en] dictionary: 10152 types
2022-12-15 23:55:28 | INFO | fairseq_cli.train | TransformerModel(
2022-12-15 23:55:28 | INFO | fairseq_cli.train | task: MoETranslationTask
2022-12-15 23:55:28 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-15 23:55:28 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-12-15 23:55:28 | INFO | fairseq_cli.train | num. shared model params: 40,848,384 (num. trained: 40,848,384)
2022-12-15 23:55:28 | INFO | fairseq_cli.train | num. expert model params: 25202688 (num. trained: 25202688)
2022-12-15 23:55:28 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14\valid.de-en.de
2022-12-15 23:55:28 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14\valid.de-en.en
2022-12-15 23:55:28 | INFO | moe_extension.tasks.moe_translation | data-bin/iwslt14 valid de-en 7283 examples
2022-12-15 23:55:28 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- encoder.layers.3.moe_layer.gate.wg.bias
2022-12-15 23:55:28 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- encoder.layers.5.moe_layer.gate.wg.bias
2022-12-15 23:55:28 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.layers.1.moe_layer.gate.wg.bias
2022-12-15 23:55:28 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.layers.3.moe_layer.gate.wg.bias
2022-12-15 23:55:28 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.layers.5.moe_layer.gate.wg.bias
2022-12-15 23:55:28 | INFO | fairseq.trainer | detected shared parameter: encoder.layers.1.moe_layer.gate.wg.bias <- decoder.output_projection.bias
2022-12-15 23:55:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-15 23:55:28 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 12.000 GB ; name = NVIDIA GeForce RTX 3060
2022-12-15 23:55:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-15 23:55:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-15 23:55:28 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2022-12-15 23:55:28 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints\checkpoint_last.pt
2022-12-15 23:55:28 | INFO | fairseq.trainer | No existing checkpoint found checkpoints\checkpoint_last.pt
2022-12-15 23:55:28 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-15 23:55:28 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14\train.de-en.de
2022-12-15 23:55:28 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14\train.de-en.en
2022-12-15 23:55:28 | INFO | moe_extension.tasks.moe_translation | data-bin/iwslt14 train de-en 160239 examples
2022-12-15 23:55:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-15 23:55:29 | INFO | fairseq.trainer | begin training epoch 1
2022-12-15 23:55:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 23:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-15 23:55:52 | INFO | train_inner | epoch 001:    101 / 1102 loss=12.623, nll_loss=12.46, aux_loss=0, ppl=5635.34, wps=16149.1, ups=4.6, wpb=3505.2, bsz=119.7, num_updates=100, lr=1.25e-05, gnorm=3.609, loss_scale=64, train_wall=22, gb_free=8.7, wall=24
2022-12-15 23:56:14 | INFO | train_inner | epoch 001:    201 / 1102 loss=10.964, nll_loss=10.607, aux_loss=0, ppl=1559.94, wps=16284.9, ups=4.57, wpb=3564.4, bsz=142.1, num_updates=200, lr=2.5e-05, gnorm=1.642, loss_scale=64, train_wall=22, gb_free=8.7, wall=46
2022-12-15 23:56:36 | INFO | train_inner | epoch 001:    301 / 1102 loss=10.11, nll_loss=9.623, aux_loss=0, ppl=788.46, wps=16296.1, ups=4.57, wpb=3563.5, bsz=133.4, num_updates=300, lr=3.75e-05, gnorm=1.656, loss_scale=64, train_wall=21, gb_free=9, wall=68
2022-12-15 23:56:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-15 23:56:58 | INFO | train_inner | epoch 001:    402 / 1102 loss=9.575, nll_loss=8.979, aux_loss=0, ppl=504.5, wps=16466.5, ups=4.47, wpb=3680, bsz=159, num_updates=400, lr=5e-05, gnorm=1.726, loss_scale=32, train_wall=22, gb_free=8.6, wall=90
2022-12-15 23:57:20 | INFO | train_inner | epoch 001:    502 / 1102 loss=9.37, nll_loss=8.721, aux_loss=0, ppl=421.97, wps=16515, ups=4.54, wpb=3635.6, bsz=152.6, num_updates=500, lr=6.25e-05, gnorm=1.932, loss_scale=32, train_wall=22, gb_free=9, wall=112
2022-12-15 23:57:42 | INFO | train_inner | epoch 001:    602 / 1102 loss=9.021, nll_loss=8.318, aux_loss=0, ppl=319.23, wps=16549.6, ups=4.6, wpb=3597.4, bsz=157.4, num_updates=600, lr=7.5e-05, gnorm=1.838, loss_scale=32, train_wall=21, gb_free=8.9, wall=134
2022-12-15 23:58:04 | INFO | train_inner | epoch 001:    702 / 1102 loss=8.783, nll_loss=8.037, aux_loss=0, ppl=262.66, wps=16055.7, ups=4.5, wpb=3567.7, bsz=160.9, num_updates=700, lr=8.75e-05, gnorm=1.981, loss_scale=32, train_wall=22, gb_free=8.7, wall=156
2022-12-15 23:58:26 | INFO | train_inner | epoch 001:    802 / 1102 loss=8.652, nll_loss=7.886, aux_loss=0, ppl=236.57, wps=16680, ups=4.65, wpb=3587.1, bsz=137, num_updates=800, lr=0.0001, gnorm=1.628, loss_scale=32, train_wall=21, gb_free=8.8, wall=178
2022-12-15 23:58:47 | INFO | train_inner | epoch 001:    902 / 1102 loss=8.471, nll_loss=7.675, aux_loss=0, ppl=204.43, wps=16515, ups=4.66, wpb=3547.7, bsz=140.4, num_updates=900, lr=0.0001125, gnorm=1.714, loss_scale=32, train_wall=21, gb_free=8.9, wall=199
2022-12-15 23:59:09 | INFO | train_inner | epoch 001:   1002 / 1102 loss=8.315, nll_loss=7.495, aux_loss=0, ppl=180.41, wps=16398.9, ups=4.55, wpb=3601.3, bsz=154.1, num_updates=1000, lr=0.000125, gnorm=1.791, loss_scale=32, train_wall=22, gb_free=8.9, wall=221
2022-12-15 23:59:30 | INFO | train_inner | epoch 001:   1102 / 1102 loss=8.216, nll_loss=7.381, aux_loss=0, ppl=166.65, wps=16681, ups=4.68, wpb=3561.5, bsz=141.5, num_updates=1100, lr=0.0001375, gnorm=1.681, loss_scale=32, train_wall=21, gb_free=8.9, wall=242
2022-12-15 23:59:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 00:04:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.118 | nll_loss 7.261 | aux_loss 0 | ppl 153.4 | bleu 3.92 | wps 609.6 | wpb 2835.3 | bsz 115.6 | num_updates 1100
2022-12-16 00:04:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1100 updates
2022-12-16 00:04:21 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint1.pt
2022-12-16 00:04:23 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint1.pt
2022-12-16 00:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint1.pt (epoch 1 @ 1100 updates, score 3.92) (writing took 2.9107510000000048 seconds)
2022-12-16 00:04:24 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-12-16 00:04:24 | INFO | train | epoch 001 | loss 9.457 | nll_loss 8.828 | aux_loss 0 | ppl 454.42 | wps 7378.4 | ups 2.06 | wpb 3582.9 | bsz 145.3 | num_updates 1100 | lr 0.0001375 | gnorm 1.927 | loss_scale 32 | train_wall 237 | gb_free 8.9 | wall 536
2022-12-16 00:04:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 00:04:24 | INFO | fairseq.trainer | begin training epoch 2
2022-12-16 00:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:04:47 | INFO | train_inner | epoch 002:    100 / 1102 loss=7.952, nll_loss=7.079, aux_loss=0, ppl=135.24, wps=1153.1, ups=0.32, wpb=3646.3, bsz=162, num_updates=1200, lr=0.00015, gnorm=1.561, loss_scale=32, train_wall=22, gb_free=8.9, wall=559
2022-12-16 00:05:08 | INFO | train_inner | epoch 002:    200 / 1102 loss=7.924, nll_loss=7.044, aux_loss=0, ppl=131.98, wps=16469.7, ups=4.6, wpb=3581, bsz=144.6, num_updates=1300, lr=0.0001625, gnorm=1.61, loss_scale=32, train_wall=21, gb_free=8.7, wall=580
2022-12-16 00:05:30 | INFO | train_inner | epoch 002:    300 / 1102 loss=7.726, nll_loss=6.818, aux_loss=0, ppl=112.84, wps=16336.5, ups=4.61, wpb=3540.3, bsz=161.2, num_updates=1400, lr=0.000175, gnorm=1.69, loss_scale=32, train_wall=21, gb_free=8.8, wall=602
2022-12-16 00:05:52 | INFO | train_inner | epoch 002:    400 / 1102 loss=7.748, nll_loss=6.839, aux_loss=0, ppl=114.47, wps=16610, ups=4.65, wpb=3570.1, bsz=129.4, num_updates=1500, lr=0.0001875, gnorm=1.542, loss_scale=32, train_wall=21, gb_free=8.9, wall=624
2022-12-16 00:06:13 | INFO | train_inner | epoch 002:    500 / 1102 loss=7.493, nll_loss=6.55, aux_loss=0, ppl=93.67, wps=16631, ups=4.61, wpb=3603.8, bsz=140.3, num_updates=1600, lr=0.0002, gnorm=1.52, loss_scale=32, train_wall=21, gb_free=8.8, wall=645
2022-12-16 00:06:35 | INFO | train_inner | epoch 002:    600 / 1102 loss=7.336, nll_loss=6.37, aux_loss=0, ppl=82.7, wps=16628.7, ups=4.63, wpb=3591.6, bsz=148.4, num_updates=1700, lr=0.0002125, gnorm=1.604, loss_scale=32, train_wall=21, gb_free=8.8, wall=667
2022-12-16 00:06:56 | INFO | train_inner | epoch 002:    700 / 1102 loss=7.172, nll_loss=6.179, aux_loss=0, ppl=72.48, wps=16469.5, ups=4.64, wpb=3547.1, bsz=149, num_updates=1800, lr=0.000225, gnorm=1.573, loss_scale=32, train_wall=21, gb_free=8.7, wall=688
2022-12-16 00:07:18 | INFO | train_inner | epoch 002:    800 / 1102 loss=7.037, nll_loss=6.024, aux_loss=0, ppl=65.07, wps=16622.6, ups=4.61, wpb=3607.1, bsz=146.7, num_updates=1900, lr=0.0002375, gnorm=1.538, loss_scale=32, train_wall=21, gb_free=8.7, wall=710
2022-12-16 00:07:40 | INFO | train_inner | epoch 002:    900 / 1102 loss=6.991, nll_loss=5.968, aux_loss=0, ppl=62.6, wps=16752.6, ups=4.65, wpb=3600.7, bsz=137.8, num_updates=2000, lr=0.00025, gnorm=1.511, loss_scale=32, train_wall=21, gb_free=8.9, wall=732
2022-12-16 00:08:01 | INFO | train_inner | epoch 002:   1000 / 1102 loss=6.859, nll_loss=5.816, aux_loss=0, ppl=56.32, wps=16369.9, ups=4.66, wpb=3515.5, bsz=141.3, num_updates=2100, lr=0.0002625, gnorm=1.516, loss_scale=32, train_wall=21, gb_free=8.9, wall=753
2022-12-16 00:08:22 | INFO | train_inner | epoch 002:   1100 / 1102 loss=6.767, nll_loss=5.708, aux_loss=0, ppl=52.29, wps=17033.8, ups=4.7, wpb=3624.9, bsz=140.6, num_updates=2200, lr=0.000275, gnorm=1.513, loss_scale=32, train_wall=21, gb_free=8.8, wall=774
2022-12-16 00:08:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 00:08:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 00:10:43 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.494 | nll_loss 5.376 | aux_loss 0 | ppl 41.54 | bleu 9.84 | wps 1276.7 | wpb 2835.3 | bsz 115.6 | num_updates 2202 | best_bleu 9.84
2022-12-16 00:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2202 updates
2022-12-16 00:10:43 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint2.pt
2022-12-16 00:10:45 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint2.pt
2022-12-16 00:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint2.pt (epoch 2 @ 2202 updates, score 9.84) (writing took 3.0434626999999637 seconds)
2022-12-16 00:10:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-12-16 00:10:46 | INFO | train | epoch 002 | loss 7.365 | nll_loss 6.4 | aux_loss 0 | ppl 84.46 | wps 10340.5 | ups 2.89 | wpb 3583.6 | bsz 145.4 | num_updates 2202 | lr 0.00027525 | gnorm 1.562 | loss_scale 32 | train_wall 234 | gb_free 8.8 | wall 918
2022-12-16 00:10:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 00:10:46 | INFO | fairseq.trainer | begin training epoch 3
2022-12-16 00:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:11:08 | INFO | train_inner | epoch 003:     98 / 1102 loss=6.538, nll_loss=5.45, aux_loss=0, ppl=43.7, wps=2175.9, ups=0.6, wpb=3604.3, bsz=140.9, num_updates=2300, lr=0.0002875, gnorm=1.419, loss_scale=32, train_wall=22, gb_free=8.8, wall=940
2022-12-16 00:11:30 | INFO | train_inner | epoch 003:    198 / 1102 loss=6.437, nll_loss=5.333, aux_loss=0, ppl=40.32, wps=16468.8, ups=4.64, wpb=3551.7, bsz=139.6, num_updates=2400, lr=0.0003, gnorm=1.404, loss_scale=32, train_wall=21, gb_free=8.8, wall=962
2022-12-16 00:11:52 | INFO | train_inner | epoch 003:    298 / 1102 loss=6.336, nll_loss=5.215, aux_loss=0, ppl=37.15, wps=16231, ups=4.51, wpb=3595.1, bsz=146.2, num_updates=2500, lr=0.0003125, gnorm=1.438, loss_scale=32, train_wall=22, gb_free=8.6, wall=984
2022-12-16 00:12:13 | INFO | train_inner | epoch 003:    398 / 1102 loss=6.228, nll_loss=5.087, aux_loss=0, ppl=33.99, wps=17177.3, ups=4.76, wpb=3607.5, bsz=141.5, num_updates=2600, lr=0.000325, gnorm=1.366, loss_scale=32, train_wall=21, gb_free=8.8, wall=1005
2022-12-16 00:12:34 | INFO | train_inner | epoch 003:    498 / 1102 loss=6.187, nll_loss=5.039, aux_loss=0, ppl=32.88, wps=17022.5, ups=4.74, wpb=3594.3, bsz=134.9, num_updates=2700, lr=0.0003375, gnorm=1.338, loss_scale=32, train_wall=21, gb_free=8.7, wall=1026
2022-12-16 00:12:55 | INFO | train_inner | epoch 003:    598 / 1102 loss=6.002, nll_loss=4.829, aux_loss=0, ppl=28.41, wps=16721.3, ups=4.75, wpb=3521.5, bsz=150.2, num_updates=2800, lr=0.00035, gnorm=1.371, loss_scale=32, train_wall=21, gb_free=8.9, wall=1047
2022-12-16 00:13:16 | INFO | train_inner | epoch 003:    698 / 1102 loss=5.853, nll_loss=4.655, aux_loss=0, ppl=25.19, wps=16828.9, ups=4.66, wpb=3611.9, bsz=159.4, num_updates=2900, lr=0.0003625, gnorm=1.337, loss_scale=32, train_wall=21, gb_free=8.6, wall=1068
2022-12-16 00:13:38 | INFO | train_inner | epoch 003:    798 / 1102 loss=5.814, nll_loss=4.609, aux_loss=0, ppl=24.39, wps=16979.1, ups=4.72, wpb=3601, bsz=154.4, num_updates=3000, lr=0.000375, gnorm=1.333, loss_scale=32, train_wall=21, gb_free=8.7, wall=1090
2022-12-16 00:13:59 | INFO | train_inner | epoch 003:    898 / 1102 loss=5.9, nll_loss=4.705, aux_loss=0, ppl=26.07, wps=16894.5, ups=4.77, wpb=3541.2, bsz=124, num_updates=3100, lr=0.0003875, gnorm=1.345, loss_scale=32, train_wall=21, gb_free=8.6, wall=1110
2022-12-16 00:14:20 | INFO | train_inner | epoch 003:    998 / 1102 loss=5.734, nll_loss=4.514, aux_loss=0, ppl=22.85, wps=17070.1, ups=4.7, wpb=3633.6, bsz=152.4, num_updates=3200, lr=0.0004, gnorm=1.383, loss_scale=32, train_wall=21, gb_free=8.6, wall=1132
2022-12-16 00:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-16 00:14:41 | INFO | train_inner | epoch 003:   1099 / 1102 loss=5.646, nll_loss=4.41, aux_loss=0, ppl=21.26, wps=16719.4, ups=4.71, wpb=3546.1, bsz=154.8, num_updates=3300, lr=0.0004125, gnorm=1.357, loss_scale=16, train_wall=21, gb_free=9.1, wall=1153
2022-12-16 00:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset

2022-12-16 00:16:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.571 | nll_loss 4.259 | aux_loss 0 | ppl 19.14 | bleu 14.85 | wps 2127.2 | wpb 2835.3 | bsz 115.6 | num_updates 3303 | best_bleu 14.85
2022-12-16 00:16:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3303 updates
2022-12-16 00:16:06 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint3.pt
2022-12-16 00:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint3.pt
2022-12-16 00:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint3.pt (epoch 3 @ 3303 updates, score 14.85) (writing took 3.175080600000001 seconds)
2022-12-16 00:16:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-12-16 00:16:09 | INFO | train | epoch 003 | loss 6.058 | nll_loss 4.891 | aux_loss 0 | ppl 29.66 | wps 12234 | ups 3.41 | wpb 3584.2 | bsz 145.5 | num_updates 3303 | lr 0.000412875 | gnorm 1.371 | loss_scale 16 | train_wall 231 | gb_free 8.6 | wall 1241
2022-12-16 00:16:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 00:16:09 | INFO | fairseq.trainer | begin training epoch 4
2022-12-16 00:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:16:30 | INFO | train_inner | epoch 004:     97 / 1102 loss=5.411, nll_loss=4.143, aux_loss=0, ppl=17.67, wps=3307.5, ups=0.92, wpb=3593, bsz=151.8, num_updates=3400, lr=0.000425, gnorm=1.239, loss_scale=16, train_wall=21, gb_free=9, wall=1262
2022-12-16 00:16:51 | INFO | train_inner | epoch 004:    197 / 1102 loss=5.44, nll_loss=4.173, aux_loss=0, ppl=18.04, wps=16843.7, ups=4.76, wpb=3540.6, bsz=135.7, num_updates=3500, lr=0.0004375, gnorm=1.272, loss_scale=16, train_wall=21, gb_free=8.9, wall=1283
2022-12-16 00:17:13 | INFO | train_inner | epoch 004:    297 / 1102 loss=5.383, nll_loss=4.106, aux_loss=0, ppl=17.22, wps=16299.7, ups=4.48, wpb=3641, bsz=131.1, num_updates=3600, lr=0.00045, gnorm=1.248, loss_scale=16, train_wall=22, gb_free=9.2, wall=1305
2022-12-16 00:17:34 | INFO | train_inner | epoch 004:    397 / 1102 loss=5.372, nll_loss=4.092, aux_loss=0, ppl=17.06, wps=16556.5, ups=4.72, wpb=3508.3, bsz=136.9, num_updates=3700, lr=0.0004625, gnorm=1.259, loss_scale=16, train_wall=21, gb_free=8.7, wall=1326
2022-12-16 00:17:56 | INFO | train_inner | epoch 004:    497 / 1102 loss=5.326, nll_loss=4.036, aux_loss=0, ppl=16.41, wps=16713, ups=4.63, wpb=3610.4, bsz=138.5, num_updates=3800, lr=0.000475, gnorm=1.247, loss_scale=16, train_wall=21, gb_free=9.1, wall=1348
2022-12-16 00:17:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-16 00:18:18 | INFO | train_inner | epoch 004:    598 / 1102 loss=5.246, nll_loss=3.946, aux_loss=0, ppl=15.42, wps=16550.3, ups=4.6, wpb=3600.7, bsz=144, num_updates=3900, lr=0.0004875, gnorm=1.213, loss_scale=8, train_wall=21, gb_free=8.7, wall=1370
2022-12-16 00:18:40 | INFO | train_inner | epoch 004:    698 / 1102 loss=5.238, nll_loss=3.936, aux_loss=0, ppl=15.3, wps=16145.2, ups=4.47, wpb=3613.7, bsz=143.4, num_updates=4000, lr=0.0005, gnorm=1.214, loss_scale=8, train_wall=22, gb_free=8.7, wall=1392
2022-12-16 00:19:02 | INFO | train_inner | epoch 004:    798 / 1102 loss=5.203, nll_loss=3.896, aux_loss=0, ppl=14.89, wps=16296.3, ups=4.63, wpb=3518.5, bsz=159.8, num_updates=4100, lr=0.000493865, gnorm=1.347, loss_scale=8, train_wall=21, gb_free=8.9, wall=1413
2022-12-16 00:19:23 | INFO | train_inner | epoch 004:    898 / 1102 loss=5.133, nll_loss=3.814, aux_loss=0, ppl=14.06, wps=16418, ups=4.58, wpb=3585.6, bsz=155.3, num_updates=4200, lr=0.00048795, gnorm=1.203, loss_scale=8, train_wall=21, gb_free=8.9, wall=1435
2022-12-16 00:19:45 | INFO | train_inner | epoch 004:    998 / 1102 loss=5.081, nll_loss=3.755, aux_loss=0, ppl=13.51, wps=17120.7, ups=4.7, wpb=3643.8, bsz=144.8, num_updates=4300, lr=0.000482243, gnorm=1.13, loss_scale=8, train_wall=21, gb_free=8.7, wall=1457
2022-12-16 00:20:06 | INFO | train_inner | epoch 004:   1098 / 1102 loss=4.976, nll_loss=3.638, aux_loss=0, ppl=12.45, wps=16711.4, ups=4.66, wpb=3587.5, bsz=159, num_updates=4400, lr=0.000476731, gnorm=1.135, loss_scale=8, train_wall=21, gb_free=8.8, wall=1478
2022-12-16 00:20:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 00:21:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.961 | nll_loss 3.535 | aux_loss 0 | ppl 11.59 | bleu 24.74 | wps 1798.2 | wpb 2835.3 | bsz 115.6 | num_updates 4404 | best_bleu 24.74
2022-12-16 00:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4404 updates
2022-12-16 00:21:46 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint4.pt
2022-12-16 00:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint4.pt
2022-12-16 00:21:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint4.pt (epoch 4 @ 4404 updates, score 24.74) (writing took 3.256939500000044 seconds)
2022-12-16 00:21:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-12-16 00:21:49 | INFO | train | epoch 004 | loss 5.254 | nll_loss 3.956 | aux_loss 0 | ppl 15.52 | wps 11586.3 | ups 3.23 | wpb 3584.5 | bsz 145.5 | num_updates 4404 | lr 0.000476515 | gnorm 1.228 | loss_scale 8 | train_wall 234 | gb_free 8.7 | wall 1581
2022-12-16 00:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 00:21:50 | INFO | fairseq.trainer | begin training epoch 5
2022-12-16 00:21:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:22:10 | INFO | train_inner | epoch 005:     96 / 1102 loss=4.857, nll_loss=3.502, aux_loss=0, ppl=11.33, wps=2896.9, ups=0.81, wpb=3596.8, bsz=155, num_updates=4500, lr=0.000471405, gnorm=1.168, loss_scale=8, train_wall=21, gb_free=9, wall=1602
2022-12-16 00:22:32 | INFO | train_inner | epoch 005:    196 / 1102 loss=4.761, nll_loss=3.392, aux_loss=0, ppl=10.5, wps=16278.7, ups=4.51, wpb=3610.8, bsz=150.8, num_updates=4600, lr=0.000466252, gnorm=1.055, loss_scale=8, train_wall=22, gb_free=8.8, wall=1624
2022-12-16 00:22:54 | INFO | train_inner | epoch 005:    296 / 1102 loss=4.814, nll_loss=3.45, aux_loss=0, ppl=10.93, wps=16833.7, ups=4.67, wpb=3602.3, bsz=142.6, num_updates=4700, lr=0.000461266, gnorm=1.098, loss_scale=8, train_wall=21, gb_free=9.1, wall=1646
2022-12-16 00:23:15 | INFO | train_inner | epoch 005:    396 / 1102 loss=4.732, nll_loss=3.358, aux_loss=0, ppl=10.25, wps=16658.9, ups=4.71, wpb=3538.7, bsz=145.8, num_updates=4800, lr=0.000456435, gnorm=1.036, loss_scale=8, train_wall=21, gb_free=8.9, wall=1667
2022-12-16 00:23:36 | INFO | train_inner | epoch 005:    496 / 1102 loss=4.843, nll_loss=3.483, aux_loss=0, ppl=11.18, wps=16584.2, ups=4.7, wpb=3527.1, bsz=126.3, num_updates=4900, lr=0.000451754, gnorm=1.133, loss_scale=8, train_wall=21, gb_free=8.8, wall=1688
2022-12-16 00:23:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-16 00:23:58 | INFO | train_inner | epoch 005:    597 / 1102 loss=4.783, nll_loss=3.416, aux_loss=0, ppl=10.67, wps=16338.5, ups=4.64, wpb=3520, bsz=135.7, num_updates=5000, lr=0.000447214, gnorm=1.09, loss_scale=4, train_wall=21, gb_free=8.8, wall=1710
2022-12-16 00:24:20 | INFO | train_inner | epoch 005:    697 / 1102 loss=4.587, nll_loss=3.193, aux_loss=0, ppl=9.14, wps=17051.7, ups=4.62, wpb=3692.5, bsz=164.6, num_updates=5100, lr=0.000442807, gnorm=1.009, loss_scale=4, train_wall=21, gb_free=9, wall=1732
2022-12-16 00:24:41 | INFO | train_inner | epoch 005:    797 / 1102 loss=4.695, nll_loss=3.316, aux_loss=0, ppl=9.96, wps=16737.1, ups=4.67, wpb=3583.8, bsz=139.1, num_updates=5200, lr=0.000438529, gnorm=1.011, loss_scale=4, train_wall=21, gb_free=8.9, wall=1753
2022-12-16 00:25:02 | INFO | train_inner | epoch 005:    897 / 1102 loss=4.611, nll_loss=3.222, aux_loss=0, ppl=9.33, wps=16587.1, ups=4.68, wpb=3541.6, bsz=156.6, num_updates=5300, lr=0.000434372, gnorm=1.043, loss_scale=4, train_wall=21, gb_free=8.9, wall=1774
2022-12-16 00:25:24 | INFO | train_inner | epoch 005:    997 / 1102 loss=4.671, nll_loss=3.289, aux_loss=0, ppl=9.77, wps=16797.8, ups=4.69, wpb=3579.3, bsz=134.2, num_updates=5400, lr=0.000430331, gnorm=1.018, loss_scale=4, train_wall=21, gb_free=8.8, wall=1796
2022-12-16 00:25:45 | INFO | train_inner | epoch 005:   1097 / 1102 loss=4.58, nll_loss=3.186, aux_loss=0, ppl=9.1, wps=16889.6, ups=4.65, wpb=3636, bsz=152.2, num_updates=5500, lr=0.000426401, gnorm=1.008, loss_scale=4, train_wall=21, gb_free=9, wall=1817
2022-12-16 00:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 00:27:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.634 | nll_loss 3.149 | aux_loss 0 | ppl 8.87 | bleu 27.89 | wps 2049.1 | wpb 2835.3 | bsz 115.6 | num_updates 5505 | best_bleu 27.89
2022-12-16 00:27:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5505 updates
2022-12-16 00:27:13 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint5.pt
2022-12-16 00:27:15 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint5.pt
2022-12-16 00:27:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint5.pt (epoch 5 @ 5505 updates, score 27.89) (writing took 3.2833517999999913 seconds)
2022-12-16 00:27:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-12-16 00:27:17 | INFO | train | epoch 005 | loss 4.719 | nll_loss 3.344 | aux_loss 0 | ppl 10.15 | wps 12062.1 | ups 3.37 | wpb 3584.4 | bsz 145.5 | num_updates 5505 | lr 0.000426208 | gnorm 1.061 | loss_scale 4 | train_wall 232 | gb_free 9.1 | wall 1909
2022-12-16 00:27:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 00:27:17 | INFO | fairseq.trainer | begin training epoch 6
2022-12-16 00:27:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:27:37 | INFO | train_inner | epoch 006:     95 / 1102 loss=4.472, nll_loss=3.065, aux_loss=0, ppl=8.37, wps=3134.1, ups=0.89, wpb=3516.1, bsz=140.6, num_updates=5600, lr=0.000422577, gnorm=1.045, loss_scale=4, train_wall=21, gb_free=8.9, wall=1929
2022-12-16 00:27:59 | INFO | train_inner | epoch 006:    195 / 1102 loss=4.428, nll_loss=3.011, aux_loss=0, ppl=8.06, wps=16696.2, ups=4.72, wpb=3535.9, bsz=154.2, num_updates=5700, lr=0.000418854, gnorm=1.017, loss_scale=4, train_wall=21, gb_free=8.7, wall=1950
2022-12-16 00:28:20 | INFO | train_inner | epoch 006:    295 / 1102 loss=4.427, nll_loss=3.01, aux_loss=0, ppl=8.06, wps=16677.5, ups=4.72, wpb=3531, bsz=137.5, num_updates=5800, lr=0.000415227, gnorm=0.989, loss_scale=4, train_wall=21, gb_free=8.8, wall=1972
2022-12-16 00:28:41 | INFO | train_inner | epoch 006:    395 / 1102 loss=4.34, nll_loss=2.911, aux_loss=0, ppl=7.52, wps=16897.6, ups=4.64, wpb=3642.2, bsz=152.8, num_updates=5900, lr=0.000411693, gnorm=0.947, loss_scale=4, train_wall=21, gb_free=8.7, wall=1993
2022-12-16 00:29:03 | INFO | train_inner | epoch 006:    495 / 1102 loss=4.39, nll_loss=2.968, aux_loss=0, ppl=7.82, wps=16991.9, ups=4.66, wpb=3646.2, bsz=143, num_updates=6000, lr=0.000408248, gnorm=0.948, loss_scale=4, train_wall=21, gb_free=9, wall=2015
2022-12-16 00:29:24 | INFO | train_inner | epoch 006:    595 / 1102 loss=4.466, nll_loss=3.055, aux_loss=0, ppl=8.31, wps=16685.7, ups=4.72, wpb=3533.4, bsz=129, num_updates=6100, lr=0.000404888, gnorm=0.998, loss_scale=4, train_wall=21, gb_free=8.8, wall=2036
2022-12-16 00:29:45 | INFO | train_inner | epoch 006:    695 / 1102 loss=4.361, nll_loss=2.937, aux_loss=0, ppl=7.66, wps=16692.7, ups=4.68, wpb=3564.2, bsz=159, num_updates=6200, lr=0.00040161, gnorm=0.984, loss_scale=4, train_wall=21, gb_free=8.8, wall=2057
2022-12-16 00:30:07 | INFO | train_inner | epoch 006:    795 / 1102 loss=4.272, nll_loss=2.836, aux_loss=0, ppl=7.14, wps=16908.2, ups=4.58, wpb=3688.2, bsz=167.2, num_updates=6300, lr=0.00039841, gnorm=0.911, loss_scale=4, train_wall=21, gb_free=8.8, wall=2079
2022-12-16 00:30:28 | INFO | train_inner | epoch 006:    895 / 1102 loss=4.435, nll_loss=3.02, aux_loss=0, ppl=8.11, wps=17010.7, ups=4.77, wpb=3569.4, bsz=132.9, num_updates=6400, lr=0.000395285, gnorm=0.985, loss_scale=4, train_wall=21, gb_free=8.9, wall=2100
2022-12-16 00:30:49 | INFO | train_inner | epoch 006:    995 / 1102 loss=4.399, nll_loss=2.981, aux_loss=0, ppl=7.89, wps=16738.4, ups=4.68, wpb=3579, bsz=138, num_updates=6500, lr=0.000392232, gnorm=0.964, loss_scale=4, train_wall=21, gb_free=8.9, wall=2121
2022-12-16 00:31:11 | INFO | train_inner | epoch 006:   1095 / 1102 loss=4.35, nll_loss=2.925, aux_loss=0, ppl=7.59, wps=16630.1, ups=4.62, wpb=3602.9, bsz=143, num_updates=6600, lr=0.000389249, gnorm=0.955, loss_scale=4, train_wall=21, gb_free=8.8, wall=2143
2022-12-16 00:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2022-12-16 00:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2022-12-16 00:32:38 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.489 | nll_loss 2.98 | aux_loss 0 | ppl 7.89 | bleu 28.46 | wps 2085.8 | wpb 2835.3 | bsz 115.6 | num_updates 6607 | best_bleu 28.46
2022-12-16 00:32:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6607 updates
2022-12-16 00:32:38 | INFO | fairseq.trainer | Saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint6.pt
2022-12-16 00:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to D:\nlper\nmt\fairseq\checkpoints\checkpoint6.pt
2022-12-16 00:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints\checkpoint6.pt (epoch 6 @ 6607 updates, score 28.46) (writing took 3.577428399999917 seconds)
2022-12-16 00:32:42 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-12-16 00:32:42 | INFO | train | epoch 006 | loss 4.392 | nll_loss 2.971 | aux_loss 0 | ppl 7.84 | wps 12140.2 | ups 3.39 | wpb 3583.6 | bsz 145.4 | num_updates 6607 | lr 0.000389043 | gnorm 0.976 | loss_scale 4 | train_wall 232 | gb_free 8.7 | wall 2234
2022-12-16 00:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-12-16 00:32:42 | INFO | fairseq.trainer | begin training epoch 7
2022-12-16 00:32:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:33:02 | INFO | train_inner | epoch 007:     93 / 1102 loss=4.159, nll_loss=2.707, aux_loss=0, ppl=6.53, wps=3212.9, ups=0.9, wpb=3570.8, bsz=147.1, num_updates=6700, lr=0.000386334, gnorm=0.928, loss_scale=4, train_wall=21, gb_free=9.1, wall=2254
2022-12-16 00:33:24 | INFO | train_inner | epoch 007:    193 / 1102 loss=4.115, nll_loss=2.657, aux_loss=0, ppl=6.31, wps=16765.9, ups=4.63, wpb=3617.4, bsz=161.3, num_updates=6800, lr=0.000383482, gnorm=0.902, loss_scale=4, train_wall=21, gb_free=8.9, wall=2276
2022-12-16 00:33:45 | INFO | train_inner | epoch 007:    293 / 1102 loss=4.148, nll_loss=2.692, aux_loss=0, ppl=6.46, wps=16803.4, ups=4.64, wpb=3622.2, bsz=148.1, num_updates=6900, lr=0.000380693, gnorm=0.919, loss_scale=4, train_wall=21, gb_free=9.1, wall=2297
2022-12-16 00:34:07 | INFO | train_inner | epoch 007:    393 / 1102 loss=4.19, nll_loss=2.741, aux_loss=0, ppl=6.68, wps=16703.3, ups=4.58, wpb=3646, bsz=137.6, num_updates=7000, lr=0.000377964, gnorm=0.928, loss_scale=4, train_wall=21, gb_free=9, wall=2319
2022-12-16 00:34:29 | INFO | train_inner | epoch 007:    493 / 1102 loss=4.209, nll_loss=2.763, aux_loss=0, ppl=6.79, wps=16228.6, ups=4.65, wpb=3488.7, bsz=140.9, num_updates=7100, lr=0.000375293, gnorm=0.97, loss_scale=4, train_wall=21, gb_free=8.6, wall=2341
2022-12-16 00:34:50 | INFO | train_inner | epoch 007:    593 / 1102 loss=4.22, nll_loss=2.774, aux_loss=0, ppl=6.84, wps=16759.3, ups=4.69, wpb=3570.5, bsz=138.6, num_updates=7200, lr=0.000372678, gnorm=0.96, loss_scale=4, train_wall=21, gb_free=9, wall=2362
2022-12-16 00:35:12 | INFO | train_inner | epoch 007:    693 / 1102 loss=4.258, nll_loss=2.818, aux_loss=0, ppl=7.05, wps=16357.7, ups=4.64, wpb=3522.7, bsz=141.8, num_updates=7300, lr=0.000370117, gnorm=1.001, loss_scale=4, train_wall=21, gb_free=8.9, wall=2383
2022-12-16 00:35:34 | INFO | train_inner | epoch 007:    793 / 1102 loss=4.18, nll_loss=2.729, aux_loss=0, ppl=6.63, wps=16419.9, ups=4.53, wpb=3628.1, bsz=144.2, num_updates=7400, lr=0.000367607, gnorm=0.927, loss_scale=4, train_wall=22, gb_free=8.7, wall=2406
2022-12-16 00:35:55 | INFO | train_inner | epoch 007:    893 / 1102 loss=4.168, nll_loss=2.718, aux_loss=0, ppl=6.58, wps=16216.2, ups=4.6, wpb=3525.8, bsz=156.8, num_updates=7500, lr=0.000365148, gnorm=0.933, loss_scale=4, train_wall=21, gb_free=9, wall=2427
2022-12-16 00:36:17 | INFO | train_inner | epoch 007:    993 / 1102 loss=4.167, nll_loss=2.716, aux_loss=0, ppl=6.57, wps=16776.2, ups=4.6, wpb=3644.8, bsz=143.1, num_updates=7600, lr=0.000362738, gnorm=0.903, loss_scale=4, train_wall=21, gb_free=8.8, wall=2449
2022-12-16 00:36:39 | INFO | train_inner | epoch 007:   1093 / 1102 loss=4.177, nll_loss=2.728, aux_loss=0, ppl=6.62, wps=16338.9, ups=4.56, wpb=3586.3, bsz=137.8, num_updates=7700, lr=0.000360375, gnorm=0.912, loss_scale=4, train_wall=22, gb_free=9, wall=2471
2